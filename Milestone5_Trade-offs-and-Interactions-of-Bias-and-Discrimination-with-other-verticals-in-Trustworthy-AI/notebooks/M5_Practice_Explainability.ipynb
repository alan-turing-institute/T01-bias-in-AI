{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd29dc62",
   "metadata": {
    "id": "fd29dc62"
   },
   "source": [
    "# **Milestone 5: Trade-offs and Interactions of Bias and Discrimination with other verticals in Trustworthy AI**\n",
    "# **PRACTICE NOTEBOOK - Explainability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec28ed",
   "metadata": {
    "id": "9fec28ed"
   },
   "source": [
    "In this pratice notebook, we explore three explainability methods in the context of the hiring algorithm example. As a reminder, a company is looking to hire a new employee. They use a machine learning algorithm to select the top candidates. The candidates are assigned either 0 if they're not selected or 1 if they are. \n",
    "\n",
    "We will explore the three following methods:\n",
    "- Permutation Feature Importance\n",
    "- Shapley Values\n",
    "- LIME (Local Interpretable Model-agnostic Explanations)\n",
    "\n",
    "We use the same data as in Milestone 3 and 4, but we consider candidates from the *Black* and *White* groups only. We consider that fairness is achieved if the Disparate Impact metric is between 0.8 ans 1.2.\n",
    "\n",
    "As a side note, we use a lightGBM classifier in this notebook from the lightgbm package. The results reported here are valid when using lightgbm version 3.3.0, but we have noticed they may differ when using other versions. If in colab, there is a line of code installing the right version to start with. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a5f447",
   "metadata": {
    "id": "e9a5f447"
   },
   "source": [
    "# 1- Setting the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e991fc",
   "metadata": {
    "id": "74e991fc"
   },
   "source": [
    "In this section, there is nothing to do, we:\n",
    "- import modules, define some functions, get raw data and pre-process it\n",
    "- split data into training/testing, define a model and train it\n",
    "- get model accuracy and disparate impact "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ceb93",
   "metadata": {
    "id": "787ceb93"
   },
   "source": [
    "## 1.1. Import modules, define some functions, get raw data and pre-process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7h9J5b6htHs_",
   "metadata": {
    "id": "7h9J5b6htHs_"
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm==3.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e16d08",
   "metadata": {
    "id": "26e16d08"
   },
   "outputs": [],
   "source": [
    "# ======================= MODULES ======================\n",
    "import pickle\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "seed = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61c9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "bunch = fetch_openml(data_id=44270)\n",
    "raw_data = bunch['frame']\n",
    "data = raw_data.copy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "n6gUdvGJPJno",
   "metadata": {
    "id": "n6gUdvGJPJno"
   },
   "outputs": [],
   "source": [
    "# ==================FUNCTION========================\n",
    "def split_data_from_df(data):\n",
    "  y = data['Label'].values\n",
    "  colX = list(data.columns[(data.columns != 'Label') & (data.columns != 'Ethnicity')])\n",
    "  X = data[colX].values\n",
    "  dem = data['Ethnicity'].copy()\n",
    "  return X,y,dem\n",
    "# ==================PRE-PROCESSING========================\n",
    "# we filter only White/Black candidate and remove all nan values\n",
    "data = data.dropna()\n",
    "data = data[(data['Ethnicity']=='White') | (data['Ethnicity']=='Black')].reset_index(drop=True)\n",
    "data = data.drop(columns=['Gender'])\n",
    "enc = LabelEncoder()\n",
    "data['Ethnicity'] = enc.fit_transform(data['Ethnicity'])\n",
    "print(enc.transform(['White','Black']))\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e35ec",
   "metadata": {
    "id": "037e35ec"
   },
   "source": [
    "## 1.2. Split data into train/test set and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67567ca",
   "metadata": {
    "id": "c67567ca"
   },
   "source": [
    "We now have the dataframe with only Black (0) and White (1) candidates left, their labels (Fail:0 or Pass:1) and their 50 features. Here we define a model and train it on a training set. Note that we use here a different model than in the previous sections (*LGBMClassifier* instead of *RidgeClassifier*) as the Shapley value pakage we will use in the next section does not support yet the *RidgeClassifier* model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60da545",
   "metadata": {
    "id": "f60da545"
   },
   "outputs": [],
   "source": [
    "# split data in train/test\n",
    "data_train, data_test = train_test_split(data,test_size = 0.3,random_state=seed)\n",
    "# get X,y, gender, ethnicity\n",
    "X,y,dem = split_data_from_df(data)\n",
    "X_train,y_train,dem_train = split_data_from_df(data_train)\n",
    "X_test,y_test,dem_test = split_data_from_df(data_test)\n",
    "# define and train model\n",
    "model = LGBMClassifier(random_state=40)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7180edde",
   "metadata": {
    "id": "7180edde"
   },
   "source": [
    "## 1.3. Evaluate model accuracy and disparate impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47440f9f",
   "metadata": {
    "id": "47440f9f"
   },
   "source": [
    "We will use the disparate impact metric as a measure of fairness in this notebook. We implement the function *get_disparate_impact* that calculate disparate impact between *Black* and *White* candidates on the predictions *y_pred* gicen the ethnicity of the candidates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c808414d",
   "metadata": {
    "id": "c808414d"
   },
   "outputs": [],
   "source": [
    "# define a function that gets the disparate impact\n",
    "def success_rate(y_pred,idx_group):\n",
    "    '''gets the success rate of a group given:\n",
    "        y_pred: (n_pred,). np array containting the predictions for all candidates (0 or 1)\n",
    "        idx_group: Indexes of the candidates belonging to the group of interest \n",
    "    '''\n",
    "    y_pred_group = y_pred[idx_group]\n",
    "    sr = (y_pred_group == 1).sum() / len(y_pred_group)\n",
    "    return sr\n",
    "def get_disparate_impact(y_pred,e):\n",
    "    '''Calculate disparate impact using predicted outcome. \n",
    "    Inputs:\n",
    "        y_pred : (n_pred,). np array containting the predictions for each candidate (0 or 1)\n",
    "        e: (n_pred,). np array containing the ethnicity of each candidate (Black:0 and White:1)\n",
    "    Output: Disparate impact\n",
    "    '''\n",
    "    # indexes corresponding to black/white candidates\n",
    "    idx_b = np.where(e == 0)[0] #black\n",
    "    idx_w = np.where(e == 1)[0] #white\n",
    "    # success rates of black/white group\n",
    "    sr_w = success_rate(y_pred,idx_w)\n",
    "    sr_b = success_rate(y_pred,idx_b)\n",
    "    return sr_b/sr_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f922cc",
   "metadata": {
    "id": "54f922cc"
   },
   "source": [
    "We evaluate the accuracy of our trained model on the test set, and using the function defined above, calculate the disparate impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63a497",
   "metadata": {
    "id": "cf63a497"
   },
   "outputs": [],
   "source": [
    "# evaluate accuracy on test\n",
    "y_pred_test = model.predict(X_test)\n",
    "acc = accuracy_score(y_test,y_pred_test)\n",
    "print(\"Accuracy is %.2f\"%acc)\n",
    "# get ethnicity for test set\n",
    "X_test,y_test,dem_test = split_data_from_df(data_test)\n",
    "e_test = np.array(dem_test)\n",
    "# get disparate impact on test set predictions\n",
    "di = get_disparate_impact(y_pred_test,e_test)\n",
    "print(\"Disparate impact is %.2f\"%di)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5595da",
   "metadata": {
    "id": "2c5595da"
   },
   "source": [
    "We get a Disparate impact of 0.80 and an accuracy of 0.69. In the next three sections, we will try to understand what particular features are driving these results (both the accuracy and disparate impact).\n",
    "\n",
    "Note that these results are valid for lightgbm version 3.3.0 and that we've noticed some differences when using other versions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a0176a",
   "metadata": {
    "id": "d8a0176a"
   },
   "source": [
    "# 2. Permutation feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca27fb4",
   "metadata": {
    "id": "9ca27fb4"
   },
   "source": [
    "In this part, we demonstrate how we will see how to use permutation feature importance in the context of this problem, and how we can apply it to fairness.\n",
    "\n",
    "As a reminder, we copy here the information on Feature Importance from the Supporting Notebook. \n",
    "\n",
    "\n",
    "Feature Importance looks to assign a score to each feature relative to its importance in the prediction. There are different methods to perform feature importance depending on the model used, as described in Jason Brownlee's article [How to Calculate Feature Importance With Python](https://machinelearningmastery.com/calculate-feature-importance-with-python/).  For instance, in a simple linear regression, the feature importance can simply be the weight times the feature value. For some more complicated or non-linear models, other model-agnostic methods are needed. One is *Permutation Feature Importance*. Let's call $X$ the feature matrix (each row i is a sample and each column j is a feature) and $e$ the error of the model when predicting for $X$. We calculate the feature imporance of feature j by:\n",
    "1. Create feature matrix $X_j$ by permuting column j\n",
    "2. Calculate the new error $e_j$ when predicting for $X_j$ \n",
    "3. The feature importance is $FI_j = e_j - e$. \n",
    "\n",
    "In other words, we randomly permute the values of one of the features and see how this impact the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318ccfd",
   "metadata": {
    "id": "f318ccfd"
   },
   "source": [
    "LGBMClassifierIn the following, we replicate the code above: we train a LGBMClassifier on a training set and evaluate the base accuracy and disparate impact on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce656f6",
   "metadata": {
    "id": "5ce656f6"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = LGBMClassifier(random_state = seed)\n",
    "# split data in train/test\n",
    "data_train, data_test = train_test_split(data,test_size = 0.3,random_state=seed)\n",
    "# get X,y,ethnicity\n",
    "X,y,dem = split_data_from_df(data)\n",
    "X_train,y_train,dem_train = split_data_from_df(data_train)\n",
    "X_test,y_test,dem_test = split_data_from_df(data_test)\n",
    "# train\n",
    "model.fit(X_train,y_train)\n",
    "# Get base accuracy and feature importance\n",
    "y_pred_test = model.predict(X_test)\n",
    "acc_base = accuracy_score(y_test,y_pred_test)\n",
    "print(\"Accuracy is %.2f\"%acc_base)\n",
    "e_test = np.array(dem_test)\n",
    "di_base = get_disparate_impact(y_pred_test,e_test)\n",
    "print(\"Disparate impact is %.2f\"%di_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c23981",
   "metadata": {
    "id": "32c23981"
   },
   "source": [
    "**Instructions:** Create a function *permute_X* that takes as an input a matrix X and the index j of a column, and that returns the same matrix but with column j randomly shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e59da1",
   "metadata": {
    "id": "d8e59da1"
   },
   "outputs": [],
   "source": [
    "def permute_X(X,j):\n",
    "    Xj = X.copy()\n",
    "    ### Do something###\n",
    "    return Xj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477982bf",
   "metadata": {
    "id": "477982bf"
   },
   "source": [
    "**Instructions:** We want to permute each feature in isolation and see the impact this has on the model's prediction. We do this on the test dataset so you will permute the columns of *X_test* in this part. Also, we want to repeat the experiment a number of times (10 in this example), so we can quantify the impact of a feature using an average rather than a single number. The steps are as follow:\n",
    "\n",
    "Repeat the following for 10 iterations (*n_iter=10*):\n",
    "For each feature (j from 0 to 49):\n",
    "- permute column j and get the permuted input matrix (using the pre-defined function)\n",
    "- get the predictions of the model on this permuted input matrix\n",
    "- calculate the accuracy and disparate impact using those predictions\n",
    "store the disparate impact and accuracy in 2 numpy array of size (n_iter,n_features). \n",
    "\n",
    "The skeleton of the code is available below. Note that this may take a few minutes to compile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52bd7018",
   "metadata": {
    "id": "52bd7018"
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "n_features = 50\n",
    "n_iter = 10\n",
    "accs = np.zeros((n_iter,n_features))\n",
    "dis = np.zeros((n_iter,n_features))\n",
    "for j in tqdm(range(n_features)):\n",
    "    for i in range(n_iter):\n",
    "        ### DO SOMETHING ###\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81ac2c",
   "metadata": {
    "id": "4c81ac2c"
   },
   "source": [
    "Because we are interested in the difference of these accuracy/disparate impact values with the base values (to isolate the impact of permuting a feature), we get numpy array with these differences directly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3679b2c",
   "metadata": {
    "id": "a3679b2c"
   },
   "outputs": [],
   "source": [
    "# difference with base\n",
    "accs_diff = accs - acc_base\n",
    "dis_diff = dis - di_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccee8c8",
   "metadata": {
    "id": "1ccee8c8"
   },
   "source": [
    "### 2.1. Original permutation feature importance\n",
    "The original permutation feature importance process looks at the difference in errors, which we measure here using the accuracy. \n",
    "\n",
    "**Instructions:** For each feature, \n",
    "- get the mean accuracy differences and standard deviations over the 10 iterations. You should end up with an array of size (50,) for the means and same for the stds (one value for each feature). We refer to these arrays as *means* and *stds*. \n",
    "- In the *means* array, negative values represent features for which, when randomly permuted, accuracy decreases. These feature therefore have the most positive impact on accuracy. On the opposite, positive values represent features that make the accuracy decrease. \n",
    "- Sort the *means* and *stds* arrays by ascending *means* values. Use argsort to get the indexes in ascending order. Note that the indexes correspond to the feature we are looking at (features 0 to 49).\n",
    "- Plot the means and stds of the 10 features with the most positive impact (with the highest negative values) and the 10 features with the most negative impact (with the highest positive values) and print to which feature they correspond to. You can use the [*plt.errorbar*](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html) function to plot the mean and std together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QEHMUrl1xnAO",
   "metadata": {
    "id": "QEHMUrl1xnAO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IYGEzpKAxnEN",
   "metadata": {
    "id": "IYGEzpKAxnEN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70819db6",
   "metadata": {
    "id": "70819db6"
   },
   "source": [
    "You should get the following graph:\n",
    "<img src=\"../img/practice_img1.png\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "\n",
    "========indexes of 10 highest positive impact features=========<br>\n",
    "[ 1  3 26 37 31 28  0 17 48  5]<br>\n",
    "========indexes of 10 highest negative impact features=========<br>\n",
    "[33 45 32  6 39 47 49 27 40 41]<br>\n",
    "\n",
    "Using this approach, it looks like feature 1 and 3 (when features are numbered from 0 to 49) are the most impactful on the accuracy: randomly shuffling these bring the accuray down by a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c92197",
   "metadata": {
    "id": "21c92197"
   },
   "source": [
    "### 2.2. Permutation feature importance using disparate impact\n",
    "\n",
    "Now we directly want to see the effect of permuting features on the disparate impact metric. \n",
    "\n",
    "**Instructions:** Repeat the steps for plotting effect on accuracy but using disparate impact instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qm00kIgFxpQa",
   "metadata": {
    "id": "qm00kIgFxpQa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jq4GdvXxxpYP",
   "metadata": {
    "id": "jq4GdvXxxpYP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b49867cf",
   "metadata": {
    "id": "b49867cf"
   },
   "source": [
    "You should now get the following graph:\n",
    "<img src=\"../img/practice_img2.png\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "\n",
    "========indexes of 10 highest positive impact features========= <br>\n",
    "[ 2 22 38 39 29 31  6 19 28  9] <br>\n",
    "========indexes of 10 highest negative impact features========= <br>\n",
    "[ 3  1 14 10 26  8 45 35 48 21]\n",
    "\n",
    "\n",
    "Again, feature 3 and 1 look like they have the biggest impact. In the following section, we will try to remove feature 3 before training the model to see the effect this will have on the disparate impact. According to this, it should increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b6ada",
   "metadata": {
    "id": "524b6ada"
   },
   "source": [
    "### 2.3. Re-training the model by removing feature 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19847bbe",
   "metadata": {
    "id": "19847bbe"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = LGBMClassifier(random_state = seed)\n",
    "# split data in train/test\n",
    "data_train, data_test = train_test_split(data,test_size = 0.3,random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499571a1",
   "metadata": {
    "id": "499571a1"
   },
   "source": [
    "**Instructions:** Retrain the model but removing feature 3.\n",
    "- copy the dataframes data_train and data_test from above, and remove feature 3 (column name is a numerical 3)\n",
    "- get X_train and X_test from these new dataframes using the *split_data_from_df* function.\n",
    "- train the model on X_train\n",
    "- Predict on X_test, and evaluate the accuracy and disparate impact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DFGIwzbAxuA7",
   "metadata": {
    "id": "DFGIwzbAxuA7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fUtfAr66xuEd",
   "metadata": {
    "id": "fUtfAr66xuEd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a00b039",
   "metadata": {
    "id": "6a00b039"
   },
   "source": [
    "You should get a new accuracy of 0.68 and disparate impact of 0.84. Is this what you expected to happen? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96eb38",
   "metadata": {
    "id": "7f96eb38"
   },
   "source": [
    "# 3. SHAPLEY values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab4ed9",
   "metadata": {
    "id": "7dab4ed9"
   },
   "source": [
    "The concept of shapley values is explained in the theory notebook (supporting notebook). In a sentence, the shapley value is: \"the average marginal contribution of an instance of a feature among all possible coalitions\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fk1KcZVRvP0b",
   "metadata": {
    "id": "Fk1KcZVRvP0b"
   },
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff46b7",
   "metadata": {
    "id": "08ff46b7"
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a9b26",
   "metadata": {
    "id": "e50a9b26"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = LGBMClassifier(random_state = seed)\n",
    "# split data in train/test\n",
    "data_train, data_test = train_test_split(data,test_size = 0.3,random_state=seed)\n",
    "# get X,y,ethnicity for trai#n and test sets\n",
    "X,y,dem = split_data_from_df(data)\n",
    "X_train,y_train,dem_train = split_data_from_df(data_train)\n",
    "X_test,y_test,dem_test = split_data_from_df(data_test)\n",
    "# train model\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dcb439",
   "metadata": {
    "id": "66dcb439"
   },
   "source": [
    "**Instructions: Follow this tutorial to:**\n",
    "https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Census%20income%20classification%20with%20LightGBM.html\n",
    "- **build a tree explainer object for the model** (cell 5 in the link)\n",
    "- **get the shapley values for the features (for the entire dataset X)** (cell 5 in the link)\n",
    "- **plot the summary plot** (cell 8 in the link)\n",
    "\n",
    "Note: this can be done in 3 lines of code. \n",
    "\n",
    "**What are the 3 most influencial features for the pass/fail decision?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZSuuWpDUx1aE",
   "metadata": {
    "id": "ZSuuWpDUx1aE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZPnc1DDDx1du",
   "metadata": {
    "id": "ZPnc1DDDx1du"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c990a3eb",
   "metadata": {
    "id": "c990a3eb"
   },
   "source": [
    "Note that because there are 2 classes, there are 2 sets of shapley values. This is redundant information as there are only 2 outputs, 0 or 1 so the factors that are moving an outputs towards 1 are logically taking it away from 0 with equal contribution. We therefore only focus on the shapley values relative to 1, i.e. what features are bringing a candidate closer to or further away from success?\n",
    "\n",
    "**Instructions: Re-plot the summary plot but of *shap_values[1]*, i.e. only the second set of shapley values** A different type of graph should be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0rt256R_x7JZ",
   "metadata": {
    "id": "0rt256R_x7JZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3808dd35",
   "metadata": {
    "id": "3808dd35"
   },
   "source": [
    "Instead of a bar graph showing the mean absolute shap value, you should now get the following graph:\n",
    "<img src=\"../img/practice_img3.png\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "\n",
    "This shows the impact on model output of each observations (each dot representing the marginal contribution of our feature when added to some coalition of features). For instance for feature 1, the blue dots represent low feature values for feature 1 that correlate with a negative impact on the model output (i.e low feature values for feature 1 correlate with lower success for candidates).\n",
    "\n",
    "The SHAP values represent the impact of a feature on model output. Because we are in a classification setting, the outputs are either 0 (fail) or 1 (pass). Hence a value of 0.3 for instance will mean that 70% of the time the outcome is 0 and 30% of the time it is 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c69ab",
   "metadata": {
    "id": "bd2c69ab"
   },
   "source": [
    "Finally, we can look at the features impacting the most a specific decision (candidate i) using: *shap.plots.force(explainer.expected_value[1],shap_values[1][i])*. Note that here the subsript [1] refers to the second set of shapley values as we explained a few paragraphs above. \n",
    "\n",
    "**Instructions: Plot the force plot for candidate 3**. Note, you might need to run *shap.initjs()* before for visualisation of the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-zZw2qPix9t2",
   "metadata": {
    "id": "-zZw2qPix9t2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74341a7a",
   "metadata": {
    "id": "74341a7a"
   },
   "source": [
    "You should get this graph: \n",
    "\n",
    "<img src=\"../img/practice_img4.png\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "\n",
    "Note that the results are here in \"raw score\". Indeed, lightGBM main output is a raw score (here approximately between -4 and 2) as shown below. This raw score is then transformed by the algorithm in probability of outcome. The base value (given by explainer.expected_value[1]) is the mean output for all candidates (in terms of raw score). Then the red features show the feature that push the result upward for that candidate (Feature 30, etc) and the blue features that push results downwards (mainly Feature 3) compared to the base value. The final result of -1.22 is the sum of all the features' contributions and is the raw score for that candidate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4db3bd",
   "metadata": {
    "id": "7c4db3bd"
   },
   "outputs": [],
   "source": [
    "#raw score output\n",
    "y_pred_train_raw = model.predict(X_train,raw_score = True) # raw_score\n",
    "print(\"Raw scores ->\", y_pred_train_raw)\n",
    "#  Base value verification\n",
    "print(\"Base value is %.4f\" %y_pred_train_raw.mean())\n",
    "# raw score of candidate 3\n",
    "y_pred_raw = model.predict(X,raw_score = True) # raw_score\n",
    "print(\"Row score of candidate 3 = %.2f\" %y_pred_raw[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47077c36",
   "metadata": {
    "id": "47077c36"
   },
   "source": [
    "### Interaction with fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47680449",
   "metadata": {
    "id": "47680449"
   },
   "source": [
    "How can we use the SHAP values for fairness? Intuitively, we can look at the shap values for different groups and calculate the difference. For features with a significant difference in SHAP values, it means that the model is not using these features in the same way for different groups and that they might be causing a bias. This link (https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Explaining%20quantitative%20measures%20of%20fairness.html) explains how one can use SHAP values for fairness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727eb92",
   "metadata": {
    "id": "c727eb92"
   },
   "outputs": [],
   "source": [
    "group_mask = (data['Ethnicity']==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba8ef5a",
   "metadata": {
    "id": "4ba8ef5a"
   },
   "source": [
    "**Following the link, can you plot the *group_difference_plot* for White and Black? Use the argument *max_display=10* for readability.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rDcBrSpVyHvr",
   "metadata": {
    "id": "rDcBrSpVyHvr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46a3ff03",
   "metadata": {
    "id": "46a3ff03"
   },
   "source": [
    "**What feature most impact Black and White in different ways?** Are these results consistent with the results from Permutation Feature Importance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccb31b",
   "metadata": {
    "id": "9dccb31b"
   },
   "source": [
    "# 4. LIME (Local Interpretable Model-agnostic Explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cddcf7",
   "metadata": {
    "id": "d2cddcf7"
   },
   "source": [
    "In this final section, we will look at the explanation method LIME (Local Interpretable Model-agnostic Explanations).\n",
    "\n",
    "We do the following:\n",
    "\n",
    "- Import lime and lime_tabular\n",
    "- Define a LGBMClassifier and train it on the train set\n",
    "- Evaluate the accuracy and disparate impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HYjqf-_0v1rq",
   "metadata": {
    "id": "HYjqf-_0v1rq"
   },
   "outputs": [],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12cc3f",
   "metadata": {
    "id": "7e12cc3f"
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538f586",
   "metadata": {
    "id": "0538f586"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = LGBMClassifier(random_state = seed)\n",
    "# split data in train/test\n",
    "data_train, data_test = train_test_split(data,test_size = 0.3,random_state=seed)\n",
    "# get X,y, gender, ethnicity\n",
    "X,y,dem = split_data_from_df(data)\n",
    "X_train,y_train,dem_train = split_data_from_df(data_train)\n",
    "X_test,y_test,dem_test = split_data_from_df(data_test)\n",
    "# train\n",
    "model.fit(X_train,y_train)\n",
    "# Get base accuracy and feature importance\n",
    "y_pred_test = model.predict(X_test)\n",
    "acc_base = accuracy_score(y_test,y_pred_test)\n",
    "print(\"Accuracy is %.2f\"%acc_base)\n",
    "di_base = get_disparate_impact(y_pred_test,dem_test)\n",
    "print(\"Disparate impact is %.2f\"%di_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d06e6",
   "metadata": {
    "id": "da8d06e6"
   },
   "source": [
    "**Instructions: Create an explainer object using *lime.lime_tabular.LimeTabularExplainer*** (refer to https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xsgefx6qyLYE",
   "metadata": {
    "id": "Xsgefx6qyLYE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1b95ad3",
   "metadata": {
    "id": "b1b95ad3"
   },
   "source": [
    "**Instructions: Explain instances 0,3,56 of the test:**\n",
    "- Use *explainer.explain_instance* to create an explanation object *exp* (refer to https://lime-ml.readthedocs.io/en/latest/lime.html#lime.lime_tabular.LimeTabularExplainer.explain_instance)\n",
    "- Use the argument *num_features = 6* to limit the explaination to 6 features only\n",
    "- display using the command *exp.show_in_notebook(show_all=False)*\n",
    "\n",
    "Note: This might take a while to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8iV0sFtNyOg2",
   "metadata": {
    "id": "8iV0sFtNyOg2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BIzrQKqyyOjd",
   "metadata": {
    "id": "BIzrQKqyyOjd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DWor8E4xyOm5",
   "metadata": {
    "id": "DWor8E4xyOm5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd3f09d6",
   "metadata": {
    "id": "dd3f09d6"
   },
   "source": [
    "For instance 0 (candidate 0), you should get the following figure: \n",
    "<img src=\"../img/practice_img5.png\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "\n",
    "There are 3 parts of the graph:\n",
    "- On the left is the predicted probability that this example is classified as Fail(0) or Pass(1). \n",
    "- In the middle, the 6 features with the most important influence on the result are presented. In orange are the ones that push towards an outcome of 1, and in blue towards an outcome of 0. The bars and float numbers associated present the relative importance of these features. For instance, Feature 3 contributes 0.15 towards a prediction of 1. Next to this bar is the name of the feature concerned (here 3) and its value range (<= -0.69). Indeed, continuous features are discretized. The bin of values in which the feature falls decides of the influence of the feature. For example, candidates 0 and 3 of the test set both have feature 3 values below -0.69, which corresponds to a contribution of +0.14/0.15 towards outcome 1 instead of 0.13 towards outcome 0 for candidate 56 (with feature 3 value being above 0.70).\n",
    "- On the right, the actual feature value for the candidate is shown. \n",
    "\n",
    "Again, using LIME for local explanations, one can see that features 1 and 3 seem to be very important. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b340e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M5_Practice_Explainability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
