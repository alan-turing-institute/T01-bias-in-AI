{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMuRYKtj4SHR"
      },
      "source": [
        "# **Milestone 3: Sources, Forms, and Quantification of Bias and Discrimination in Supervised Learning**\n",
        "## **PRACTICE NOTEBOOK 2 - Evaluate model bias \"manually\"**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v83HDaM4g99"
      },
      "source": [
        "In this part of the course, we will look for bias using a practical example. A  company is looking to hire a new employee. They use a machine learning algorithm to select the top candidates. The candidates are assigned either 0 if they're not selected or 1 if they are. \n",
        "\n",
        "There are 4 practice notebooks in total (current one in red):\n",
        "1. Explore given data to: detect potential bias early & check for proxies \n",
        "2. Evaluate model bias \"manually\"\n",
        "3. Evaluate model bias using **holisticai** library\n",
        "4. <font color='red'> **Example code to get confidence intervals for a metric (nothing to do)**</font>\n",
        "\n",
        "Instructions to complete in each parts are in bold. Intermediate results are given so one can continue the exercise. \n",
        "\n",
        "This is notebook number 2. \n",
        "\n",
        "Up to this point, we haven't used any model yet and simply analysed the dataset used for training. We haven't used any fairness metrics either, but we can already see a few points that are warning signs. The first one is that the average success rate within the training data differs for various ethnicities (Asian : 0.32, Black : 0.35, Hispanic : 0.38 , White : 0.39). If there was no proxies at all for ethnicity in the data, it would be fine to train with this data as ethnicity would have zero influence on the result. Unfortunately, we have seen that it is not the case. We can therefore expect there might be some bias on a model trained on this data. \n",
        "\n",
        "Note that in a more thorough experiment, we would test fairness using cross-validation or bootsampling methods (see section 10.2 from this book), giving us more trustworthy average figures for each metric (for accuracy and fairness). In this course, we do the analysis on one single random split of the data, hence yielding only one figure. Example code to obtain a confidence interval is provided in practice notebook 4. \n",
        "\n",
        "We evaluate two types of metrics:\n",
        "\n",
        "- Equality of outcome metrics: These measure the distribution of positive outcomes with respect to the protected characteristic. We focus on this when we would like our model to predict an equal proportion of positive outcomes for the protected group compared with the rest of the population. We will estimate the two following metrics: Statistical Parity and Disparate Impact. \n",
        "\n",
        "- Equalized Odds and Equality of Opportunity metrics. These measure the distribution of model errors with respect to the protected characteristic. These are useful when Statistical Parity is not an appropriate goal, for example where there are legitimate reasons for a protected group to have a different rate of positive outcome to the rest of the population, but where we would like to make sure the model makes the same volume and types of errors for both groups. We will estimate the following metric: Equal Opportunity Difference.\n",
        "\n",
        "In this notebook we will:\n",
        "- Split data into train/test sets, define and train a model, get overall accuracy\n",
        "- Calculate the Equality of Outcome metrics *Statistical Parity* and *Disparate Impact* on the original test set\n",
        "- Calculate *Statistical Parity* and *Disparate Impact* on a rebalanced test set\n",
        "- Calculate the *Equal Opportunity Difference*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nYt5_h77QGN"
      },
      "source": [
        "## **0 - Import modules, load data and useful functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KgfZWXdy1REj"
      },
      "outputs": [],
      "source": [
        "#imports \n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn import under_sampling, over_sampling\n",
        "import imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "ClEzlOOOpr0B",
        "outputId": "cbefc6f4-2e35-49c3-a6fe-c1f5ce0136f2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>...</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>-0.876827</td>\n",
              "      <td>-0.541625</td>\n",
              "      <td>-1.968103</td>\n",
              "      <td>-0.983961</td>\n",
              "      <td>-0.844169</td>\n",
              "      <td>-1.501946</td>\n",
              "      <td>0.690312</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.688365</td>\n",
              "      <td>-0.650548</td>\n",
              "      <td>0.836874</td>\n",
              "      <td>-0.368150</td>\n",
              "      <td>-0.581680</td>\n",
              "      <td>-1.331884</td>\n",
              "      <td>0.650493</td>\n",
              "      <td>-1.568461</td>\n",
              "      <td>-0.049631</td>\n",
              "      <td>1.652508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>0.289426</td>\n",
              "      <td>-0.456504</td>\n",
              "      <td>-1.631056</td>\n",
              "      <td>1.412955</td>\n",
              "      <td>-0.024019</td>\n",
              "      <td>0.996284</td>\n",
              "      <td>-2.097603</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.086540</td>\n",
              "      <td>-0.191318</td>\n",
              "      <td>-1.355319</td>\n",
              "      <td>0.132149</td>\n",
              "      <td>0.066033</td>\n",
              "      <td>0.593083</td>\n",
              "      <td>0.586004</td>\n",
              "      <td>0.505305</td>\n",
              "      <td>1.912276</td>\n",
              "      <td>0.894592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>White</td>\n",
              "      <td>-1.586135</td>\n",
              "      <td>-1.380555</td>\n",
              "      <td>0.295020</td>\n",
              "      <td>-0.216107</td>\n",
              "      <td>-0.741104</td>\n",
              "      <td>-0.261113</td>\n",
              "      <td>-1.455017</td>\n",
              "      <td>...</td>\n",
              "      <td>0.910211</td>\n",
              "      <td>1.549717</td>\n",
              "      <td>-0.854247</td>\n",
              "      <td>-0.821632</td>\n",
              "      <td>-0.407780</td>\n",
              "      <td>0.279067</td>\n",
              "      <td>-1.077535</td>\n",
              "      <td>0.637090</td>\n",
              "      <td>-1.492060</td>\n",
              "      <td>0.663372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>1.848717</td>\n",
              "      <td>-0.366034</td>\n",
              "      <td>0.139653</td>\n",
              "      <td>-1.608005</td>\n",
              "      <td>0.919614</td>\n",
              "      <td>0.012383</td>\n",
              "      <td>-0.940962</td>\n",
              "      <td>...</td>\n",
              "      <td>0.183061</td>\n",
              "      <td>2.232990</td>\n",
              "      <td>-1.446175</td>\n",
              "      <td>-1.086500</td>\n",
              "      <td>1.233636</td>\n",
              "      <td>0.574872</td>\n",
              "      <td>-1.421603</td>\n",
              "      <td>0.784939</td>\n",
              "      <td>-0.019479</td>\n",
              "      <td>-1.510623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>1.146786</td>\n",
              "      <td>-0.813799</td>\n",
              "      <td>-0.945187</td>\n",
              "      <td>-1.291322</td>\n",
              "      <td>-0.198067</td>\n",
              "      <td>0.124782</td>\n",
              "      <td>-0.906823</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.321193</td>\n",
              "      <td>0.587776</td>\n",
              "      <td>-1.824633</td>\n",
              "      <td>-1.606074</td>\n",
              "      <td>-1.598096</td>\n",
              "      <td>0.205599</td>\n",
              "      <td>0.079420</td>\n",
              "      <td>-0.440719</td>\n",
              "      <td>-2.475596</td>\n",
              "      <td>-1.792374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>-1.238414</td>\n",
              "      <td>1.121718</td>\n",
              "      <td>1.269631</td>\n",
              "      <td>-0.093551</td>\n",
              "      <td>-0.892135</td>\n",
              "      <td>-0.497069</td>\n",
              "      <td>0.424476</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.308610</td>\n",
              "      <td>-1.147748</td>\n",
              "      <td>0.097719</td>\n",
              "      <td>1.165068</td>\n",
              "      <td>-1.349054</td>\n",
              "      <td>-0.547598</td>\n",
              "      <td>2.273229</td>\n",
              "      <td>-2.048750</td>\n",
              "      <td>0.983288</td>\n",
              "      <td>-0.034799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>White</td>\n",
              "      <td>-0.838622</td>\n",
              "      <td>-0.150014</td>\n",
              "      <td>-0.813486</td>\n",
              "      <td>-1.623939</td>\n",
              "      <td>0.935371</td>\n",
              "      <td>0.518092</td>\n",
              "      <td>-0.892630</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.032027</td>\n",
              "      <td>0.649060</td>\n",
              "      <td>-1.123258</td>\n",
              "      <td>-1.966932</td>\n",
              "      <td>-1.093828</td>\n",
              "      <td>-0.781517</td>\n",
              "      <td>0.829586</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.135866</td>\n",
              "      <td>-1.007733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>Asian</td>\n",
              "      <td>1.230613</td>\n",
              "      <td>-0.022335</td>\n",
              "      <td>-0.928522</td>\n",
              "      <td>0.141622</td>\n",
              "      <td>1.601439</td>\n",
              "      <td>-0.592846</td>\n",
              "      <td>0.333232</td>\n",
              "      <td>...</td>\n",
              "      <td>0.763746</td>\n",
              "      <td>-0.357190</td>\n",
              "      <td>-0.608629</td>\n",
              "      <td>-0.654341</td>\n",
              "      <td>2.113140</td>\n",
              "      <td>-0.528662</td>\n",
              "      <td>-0.710054</td>\n",
              "      <td>0.706443</td>\n",
              "      <td>-0.537377</td>\n",
              "      <td>0.952972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>-0.796576</td>\n",
              "      <td>-0.913765</td>\n",
              "      <td>1.229233</td>\n",
              "      <td>0.723160</td>\n",
              "      <td>-0.747698</td>\n",
              "      <td>-1.486035</td>\n",
              "      <td>-0.848035</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.312473</td>\n",
              "      <td>0.474229</td>\n",
              "      <td>-1.425753</td>\n",
              "      <td>0.018631</td>\n",
              "      <td>0.640391</td>\n",
              "      <td>0.229921</td>\n",
              "      <td>0.363351</td>\n",
              "      <td>1.078940</td>\n",
              "      <td>-0.279152</td>\n",
              "      <td>0.658600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>0.103155</td>\n",
              "      <td>1.347344</td>\n",
              "      <td>-0.046541</td>\n",
              "      <td>-0.383026</td>\n",
              "      <td>2.013982</td>\n",
              "      <td>0.199071</td>\n",
              "      <td>0.456671</td>\n",
              "      <td>...</td>\n",
              "      <td>0.811249</td>\n",
              "      <td>-0.722591</td>\n",
              "      <td>0.538860</td>\n",
              "      <td>-1.102773</td>\n",
              "      <td>-0.196343</td>\n",
              "      <td>-0.719513</td>\n",
              "      <td>0.650744</td>\n",
              "      <td>0.316991</td>\n",
              "      <td>0.969334</td>\n",
              "      <td>-1.121850</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Label  Gender Ethnicity         0         1         2         3  \\\n",
              "0       0.0  Female  Hispanic -0.876827 -0.541625 -1.968103 -0.983961   \n",
              "1       0.0    Male  Hispanic  0.289426 -0.456504 -1.631056  1.412955   \n",
              "2       0.0    Male     White -1.586135 -1.380555  0.295020 -0.216107   \n",
              "3       1.0  Female  Hispanic  1.848717 -0.366034  0.139653 -1.608005   \n",
              "4       1.0    Male  Hispanic  1.146786 -0.813799 -0.945187 -1.291322   \n",
              "...     ...     ...       ...       ...       ...       ...       ...   \n",
              "9995    1.0    Male  Hispanic -1.238414  1.121718  1.269631 -0.093551   \n",
              "9996    0.0  Female     White -0.838622 -0.150014 -0.813486 -1.623939   \n",
              "9997    0.0  Female     Asian  1.230613 -0.022335 -0.928522  0.141622   \n",
              "9998    0.0    Male  Hispanic -0.796576 -0.913765  1.229233  0.723160   \n",
              "9999    0.0  Female  Hispanic  0.103155  1.347344 -0.046541 -0.383026   \n",
              "\n",
              "             4         5         6  ...        10        11        12  \\\n",
              "0    -0.844169 -1.501946  0.690312  ... -1.688365 -0.650548  0.836874   \n",
              "1    -0.024019  0.996284 -2.097603  ... -0.086540 -0.191318 -1.355319   \n",
              "2    -0.741104 -0.261113 -1.455017  ...  0.910211  1.549717 -0.854247   \n",
              "3     0.919614  0.012383 -0.940962  ...  0.183061  2.232990 -1.446175   \n",
              "4    -0.198067  0.124782 -0.906823  ... -0.321193  0.587776 -1.824633   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "9995 -0.892135 -0.497069  0.424476  ... -0.308610 -1.147748  0.097719   \n",
              "9996  0.935371  0.518092 -0.892630  ... -1.032027  0.649060 -1.123258   \n",
              "9997  1.601439 -0.592846  0.333232  ...  0.763746 -0.357190 -0.608629   \n",
              "9998 -0.747698 -1.486035 -0.848035  ... -0.312473  0.474229 -1.425753   \n",
              "9999  2.013982  0.199071  0.456671  ...  0.811249 -0.722591  0.538860   \n",
              "\n",
              "            13        14        15        16        17        18        19  \n",
              "0    -0.368150 -0.581680 -1.331884  0.650493 -1.568461 -0.049631  1.652508  \n",
              "1     0.132149  0.066033  0.593083  0.586004  0.505305  1.912276  0.894592  \n",
              "2    -0.821632 -0.407780  0.279067 -1.077535  0.637090 -1.492060  0.663372  \n",
              "3    -1.086500  1.233636  0.574872 -1.421603  0.784939 -0.019479 -1.510623  \n",
              "4    -1.606074 -1.598096  0.205599  0.079420 -0.440719 -2.475596 -1.792374  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "9995  1.165068 -1.349054 -0.547598  2.273229 -2.048750  0.983288 -0.034799  \n",
              "9996 -1.966932 -1.093828 -0.781517  0.829586  0.000056  0.135866 -1.007733  \n",
              "9997 -0.654341  2.113140 -0.528662 -0.710054  0.706443 -0.537377  0.952972  \n",
              "9998  0.018631  0.640391  0.229921  0.363351  1.078940 -0.279152  0.658600  \n",
              "9999 -1.102773 -0.196343 -0.719513  0.650744  0.316991  0.969334 -1.121850  \n",
              "\n",
              "[10000 rows x 23 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load data\n",
        "from sklearn.datasets import fetch_openml\n",
        "bunch = fetch_openml(data_id=44270)\n",
        "raw_data = bunch['frame']\n",
        "raw_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo4na9kCw6bN"
      },
      "outputs": [],
      "source": [
        "# remove all nans --> we will use the variable data (without nans) for the remaining of this notebook \n",
        "data = raw_data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLwniYoCY6_y"
      },
      "outputs": [],
      "source": [
        "def plot_cm(y_true,y_pred,labels = [1,0],display_labels = [1,0], ax = None):\n",
        "  cm = confusion_matrix(y_true,y_pred,labels = labels)\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "  else:\n",
        "    fig = ax.figure\n",
        "  sns.heatmap(cm, annot=True, ax = ax, cmap='viridis',fmt='g')\n",
        "\n",
        "  ax.set(xticklabels=display_labels,\n",
        "          yticklabels=display_labels,\n",
        "          ylabel=\"True label\",\n",
        "          xlabel=\"Predicted label\")\n",
        "  return cm\n",
        "def split_data_from_df(data):\n",
        "  y = data['Label'].values\n",
        "  # g = data['Gender'].values\n",
        "  # e = data['Ethnicity'].values\n",
        "  X = data[np.arange(50).astype(str)].values\n",
        "  filter_col = ['Ethnicity','Gender'] + [col for col in data if str(col).startswith('Ethnicity_')] + [col for col in data if str(col).startswith('Gender_')] \n",
        "  dem = data[filter_col].copy()\n",
        "  return X,y,dem\n",
        "def encode(df):\n",
        "  g_enc = LabelEncoder()\n",
        "  e_enc = LabelEncoder()\n",
        "  df['Gender'] = g_enc.fit_transform(df['Gender'])\n",
        "  df['Ethnicity'] = e_enc.fit_transform(df['Ethnicity'])\n",
        "  return df, g_enc,e_enc\n",
        "def resample_equal(df,cat):\n",
        "  df['uid'] = df[cat] + df['Label'].astype(str)\n",
        "  enc = LabelEncoder()\n",
        "  df['uid'] = enc.fit_transform(df['uid'])\n",
        "  # Resample\n",
        "  uid = df['uid'].values\n",
        "  res = imblearn.over_sampling.RandomOverSampler(random_state=6)\n",
        "  df_res,euid = res.fit_resample(df,uid)\n",
        "  df_res = pd.DataFrame(df_res,columns = df.columns)\n",
        "  df_res = df_res.sample(frac=1).reset_index(drop=True)\n",
        "  df_res['Label'] = df_res['Label'].astype(float)\n",
        "  return df_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9aOH89iRI7q"
      },
      "source": [
        "## **1- Train the model & evaluate overall accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "her5sk9weY3N"
      },
      "source": [
        "In the following code, we split the data into a train and test set, then train the model and evaluate overall accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lng_LHndeW_J",
        "outputId": "dabbf8e9-5e26-4a8f-9524-ee9e9a269d5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy is 0.71\n"
          ]
        }
      ],
      "source": [
        "# split into train/test\n",
        "data_train, data_test = train_test_split(data,test_size = 0.3,random_state=4)\n",
        "# get X,y,demographics for each\n",
        "X_train,y_train,dem_train = split_data_from_df(data_train)\n",
        "X_test,y_test,dem_test = split_data_from_df(data_test)\n",
        "# define model and train\n",
        "model = RidgeClassifier(random_state=42)\n",
        "model.fit(X_train,y_train)\n",
        "# evaluate on test\n",
        "y_pred_test = model.predict(X_test)\n",
        "acc = accuracy_score(y_test,y_pred_test)\n",
        "print(\"Accuracy is %.2f\"%acc)\n",
        "# add predictions to data_test for simplicity of analysis\n",
        "data_test = data_test.copy()\n",
        "data_test['Pred'] = y_pred_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DHGwPQCba_0"
      },
      "source": [
        "We have an overall accuracy of 0.71, let's now calculate some of the fairness metrics described in part I. We will use:\n",
        "\n",
        "- Statistical Parity and Disparate Impact\n",
        "- Equal Odds and Equal Opportunity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaPD1_dhqLiy"
      },
      "source": [
        "## **2. Calculate Statistical Parity and Disparate Impact on original test set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMuix7ulqe5c"
      },
      "source": [
        "**Questions** : \n",
        "- **Calculate the success rate for Male, Female and White, Black, Asian, Hispanic.** Note that this is the same as the mean y value for these subsets as outcomes are either 0 or 1. \n",
        "\n",
        "- **Calculate the Statistical Parity and Disparate Impact for:** \n",
        "  - **Female vs Male**\n",
        "  - **Black vs White**\n",
        "  - **Asian vs White**\n",
        "  - **Hispanic vs White**\n",
        "\n",
        "As a reminder, here are the definition:\n",
        "  - Statistical Parity. Difference between the success rate of the minority group with the majority group. A value below zero indicates a bias. \n",
        "  - Disparate Impact. Ratio of the success rate of the minority group over the majority group. A value below 1 indicates a bias. Fairness is often considered achieved for values between 0.8 and 1.2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QAEwwiAKO3y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaMnjWE7KO81"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPEv1uWOpiGz"
      },
      "source": [
        "You should get the following results.\n",
        "\n",
        "| Tested | Statistical Parity | Disparate Impact |\n",
        "| --- | --- | ---|\n",
        "| Female vs Male |-0.07 | 0.81 |\n",
        "| Black vs White | -0.11 | 0.67 |\n",
        "| Asian vs White | -0.05 | 0.86 |\n",
        "| Hispanic vs White | -0.03 | 0.90 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzwBa_wfxYVF"
      },
      "source": [
        "**Questions** : \n",
        "- **Who experiences the most bias ? Is this in line with what you predicted in 2.2 ? Why could it be different ?** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QvORPdlxluk"
      },
      "source": [
        "## **3. Calculate Statistical Parity and Disparate Impact on a rebalanced test set**\n",
        "\n",
        "We know for the dataset analysis that the actual success rate (calculated from the ground-truth label) is lower for the *Black/Asian/Hispanic* groups compared to White group. As the test set is a random subset of the dataset, it is also likely to be true in the test set. Hence even a perfectly trained model should predict a lower success rate for those groups. To test the bias on a more balanced dataset, we resample the test set so that the success rate is equal across all groups. Note this might take a few seconds to run. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ_AriUppiG0",
        "outputId": "b1bcba4d-9404-40e9-ecae-7c8e2198998f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New success rates for rebalanced test set:\n",
            "    Asian -> 0.500\n",
            "    Black -> 0.500\n",
            "    Hispanic -> 0.500\n",
            "    White -> 0.500\n",
            "Accuracy is 0.67\n"
          ]
        }
      ],
      "source": [
        "# resample test set so success rate equal across all groups\n",
        "data_test_res = resample_equal(data_test,'Ethnicity')\n",
        "X_test_res,y_test_res,dem_test_res = split_data_from_df(data_test_res)\n",
        "# check success rates\n",
        "print(\"New success rates for rebalanced test set:\")\n",
        "pred_e_mean_res  = data_test_res.groupby('Ethnicity').mean()['Label']\n",
        "for e in pred_e_mean_res.index:\n",
        "  print('   ',e,'-> %.3f'%pred_e_mean_res[e])\n",
        "## evaluate model on rebalanced test set\n",
        "y_pred_test_res = model.predict(X_test_res)\n",
        "acc_res = accuracy_score(y_test_res,y_pred_test_res)\n",
        "print(\"Accuracy is %.2f\"%acc_res)\n",
        "## add predictions to data_test for simplicity of analysis\n",
        "data_test_res = data_test_res.copy()\n",
        "data_test_res['Pred'] = y_pred_test_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYRr0mjDpiG0"
      },
      "source": [
        "**Questions** : \n",
        "- **Repeat the steps from above to calculate the Statistical Parity and Disparate Impact on the rebalanced test set for:** \n",
        "  - **Black vs White**\n",
        "  - **Asian vs White**\n",
        "  - **Hispanic vs White**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E8n0QVxKSIC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-LlLrCBKSKo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBOxW9EnpiG0"
      },
      "source": [
        "You should get the following results.\n",
        "\n",
        "| Tested | Statistical Parity | Disparate Impact |\n",
        "| --- | --- | ---|\n",
        "| Black vs White | -0.12 | 0.68 |\n",
        "| Asian vs White | -0.02 | 0.95 |\n",
        "| Hispanic vs White | -0.04 | 0.89 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBbnU4rS2TF6"
      },
      "source": [
        "Hence even on a balanced test set there is still a bias. The disparate impact value for Black vs White is just under the \"fair\" limit of 0.8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAHEDWmBitMm"
      },
      "source": [
        "## **4. Calculate the Equal Opportunity Difference**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GFG01c8i1dC"
      },
      "source": [
        "In this section, we focus on the *Equal Opportunity Difference metric*. This metric is part of a wider category of metrics (Equalized Odds and Equality of opportunity) which compare performance/accuracy of predictions for different groups instead of success rates. \n",
        "\n",
        "Equalized Odds state that the classifer should have the same True Positive Rate (TPR) and True Negative Rate (TNR) across all groups. The true positive (negative) rate is the proportion of candidates that should be successful (unsuccessful) according to ground-truth that have been correctly classified as successful (unsuccessful) by the algorithm. Mathematically, this means that the following probabilities should be equal for all groups : \n",
        "\n",
        "$$P_{group1}(pred = \\text{0 or 1} | Y = \\text{0 or 1}) = P_{group2}(pred = \\text{0 or 1} | Y = \\text{0 or 1})$$\n",
        "\n",
        "\n",
        "The figure below illustrates the definition of TPR and TNR using a confusion matrix and their relationship with the above probabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpg26zgs8MGl"
      },
      "source": [
        "<center><img src=\"https://drive.google.com/uc?id=1bQxfIhcrNym7UC7_yzbdaUq12qCtJ9Dr\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WvYsyq_4vvr"
      },
      "source": [
        "The version of this with equal true positive rates only - i.e only looking at $P(pred = 1|y =1)$ - is called *Equal Opportunity Difference*. This measures the difference in True Positive Rates between the unprivileged and privileged group. The ideal value is 0 and fair values are often considered achieved for values between -0.1 and 0.1. In this part, we calculate the Equal Opportunity Difference for different groups. \n",
        "\n",
        "\n",
        "In the following code, we split the data into a train and test set and train the model using the train set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7N3XUjti8r_",
        "outputId": "45fdf795-a606-49fc-d087-4b5dfa6ed071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy is 0.71\n"
          ]
        }
      ],
      "source": [
        "# split into train/test\n",
        "data_train, data_test = train_test_split(data,test_size = 0.3,random_state=4)\n",
        "# get X,y,demographics for each\n",
        "X_train,y_train,dem_train = split_data_from_df(data_train)\n",
        "X_test,y_test,dem_test = split_data_from_df(data_test)\n",
        "# define model and train\n",
        "model = RidgeClassifier(random_state=42)\n",
        "model.fit(X_train,y_train)\n",
        "# evaluate on test\n",
        "y_pred_test = model.predict(X_test)\n",
        "acc = accuracy_score(y_test,y_pred_test)\n",
        "print(\"Accuracy is %.2f\"%acc)\n",
        "# add predictions to data_test for simplicity of analysis\n",
        "data_test = data_test.copy()\n",
        "data_test['Pred'] = y_pred_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxH0vOKaxnT5"
      },
      "source": [
        "**Questions:**\n",
        "- **Compute and plot the confusion matrix for Male/Female and White/Black.** Use the pre-defined function *plot_cm(y_true,y_pred)*.\n",
        "- **Compute the true positive rates for Male/Female and White/Black.** \n",
        "- **Compute the Equal Opportunity Difference as the difference of the true positive rate of the unprivileged group with the privileged one:**\n",
        "$EOD = TPR_{unprivileged}-TPR_{privileged}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxpICMgKKX8g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh0VCA1zKX-9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BjIkQZbpiG1"
      },
      "source": [
        "You should find the following:\n",
        "\n",
        "| Tested | Equal Opportunity Difference |\n",
        "| --- | ---|\n",
        "| Female vs Male | -0.09 |\n",
        "| Black vs White | -0.15 | \n",
        "| Asian vs White | -0.08 | \n",
        "| Hispanic vs White | -0.03 | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCLoZwEn-DyP"
      },
      "source": [
        "**Questions:**\n",
        "- **According to the Equal Opportunity Difference metrics, which group is subject to the most bias ?**\n",
        "- **Are these results the same as for the Disparate Impact metrics ? Is this necessarily always the case ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEtpST2RpiG1"
      },
      "source": [
        "    Answer:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-nYt5_h77QGN"
      ],
      "name": "M3_Practice_2_Evaluate fairness metrics manually.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('torch-nightly')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "f5fa74478a026ac530ef194e4df855dfb9675779484e20284ae5f690a2266d7b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
