{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M3_Practice_3_Evaluate fairness metrics with aif360.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-nYt5_h77QGN",
        "MzvvNPjWL5_-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMuRYKtj4SHR"
      },
      "source": [
        "# **Milestone 3: Sources, Forms, and Quantification of Bias and Discrimination in Supervised Learning**\n",
        "## **PRACTICE NOTEBOOK 3 - Evaluate model bias using existing libraries (aif360)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v83HDaM4g99"
      },
      "source": [
        "In this part of the course, we will look for bias using a practical example. A  company is looking to hire a new employee. They use a machine learning algorithm to select the top candidates. The candidates are assigned either 0 if they're not selected or 1 if they are. \n",
        "\n",
        "There are 4 practice notebooks in total (current one in red):\n",
        "1. Explore given data to: detect potential bias early & check for proxies \n",
        "2. Evaluate model bias \"manually\"\n",
        "3. <font color='red'> **Evaluate model bias using existing libraries (aif360)**</font>\n",
        "4. Example code to get confidence intervals for a metric (nothing to do)\n",
        "\n",
        "Instructions to complete in each parts are in bold. Intermediate results are given so one can continue the exercise. \n",
        "\n",
        "This is notebook number 3. In the previous notebook, we measured bias manually. There exist a number of python library facilitating calculations for us. In this part, we will replicate the results found in the previous section using aif360. In this notebook, we will:\n",
        "- Import data and useful modules\n",
        "- Install and import aif360\n",
        "- Learn how to create aif360 BinaryLabelDatasets object\n",
        "- Learn how to calculate fairness metrics using aif360\n",
        "\n",
        "We evaluate here the same metrics as in the previous notebook. You should get the same results as what you calculated \"manually\" in notebook 2. Here we only work with the \"Black\" unprivileged group. As a reminder, these are the results from the previous notebook that you should be able to reproduce here:\n",
        "\n",
        "| Metric | Value | \n",
        "| --- | --- | \n",
        "| Statistical Parity |-0.09 | \n",
        "| Disparate Impact | 0.75 |\n",
        "| Equal Opportunity Difference | -0.11 | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nYt5_h77QGN"
      },
      "source": [
        "## **0 - Import modules, load data and useful functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgfZWXdy1REj"
      },
      "source": [
        "#imports \n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import RidgeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ0R86MF7trN",
        "outputId": "9386cabc-e812-4b4e-ac58-d4bddb0434fa"
      },
      "source": [
        "# Only run if running on Google Colab\n",
        "!pip3 install pickle5\n",
        "import pickle5 as pickle\n",
        "!gdown --id 1-Wd1evAoDs4YsjRLfC-ifarmQL-Ozg3R # download data file from public link and place it in content/ folder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pickle5\n",
            "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30 kB 21.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 132 kB 8.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219327 sha256=d4d370f68d1319e495a1d41b12574618fcb576f83e9c6da0ed5aae2a41840a7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/6a/00/67136a90d6aca437d806d1d3cedf98106e840c97a3e5188198\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.11\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-Wd1evAoDs4YsjRLfC-ifarmQL-Ozg3R\n",
            "To: /content/data.pickle\n",
            "128MB [00:01, 89.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "ClEzlOOOpr0B",
        "outputId": "03766f65-a122-4809-c3f9-40dec57adadb"
      },
      "source": [
        "'''\n",
        "Uncomment the code below if you are running this from your local machine\n",
        "Note: Place data.pickle file in the same folder as the .ipynb notebook (Download link: https://docs.google.com/uc?export=download&id=1-Wd1evAoDs4YsjRLfC-ifarmQL-Ozg)\n",
        "'''\n",
        "# with open('data.pickle', 'rb') as handle:\n",
        "#     raw_data = pickle.load(handle)   \n",
        "if 'google.colab' in str(get_ipython()): # if running from colab\n",
        "  with open('/content/data.pickle', 'rb') as handle:\n",
        "      raw_data = pickle.load(handle)  \n",
        "raw_data[:5] #display the first 5 candidates data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>...</th>\n",
              "      <th>460</th>\n",
              "      <th>461</th>\n",
              "      <th>462</th>\n",
              "      <th>463</th>\n",
              "      <th>464</th>\n",
              "      <th>465</th>\n",
              "      <th>466</th>\n",
              "      <th>467</th>\n",
              "      <th>468</th>\n",
              "      <th>469</th>\n",
              "      <th>470</th>\n",
              "      <th>471</th>\n",
              "      <th>472</th>\n",
              "      <th>473</th>\n",
              "      <th>474</th>\n",
              "      <th>475</th>\n",
              "      <th>476</th>\n",
              "      <th>477</th>\n",
              "      <th>478</th>\n",
              "      <th>479</th>\n",
              "      <th>480</th>\n",
              "      <th>481</th>\n",
              "      <th>482</th>\n",
              "      <th>483</th>\n",
              "      <th>484</th>\n",
              "      <th>485</th>\n",
              "      <th>486</th>\n",
              "      <th>487</th>\n",
              "      <th>488</th>\n",
              "      <th>489</th>\n",
              "      <th>490</th>\n",
              "      <th>491</th>\n",
              "      <th>492</th>\n",
              "      <th>493</th>\n",
              "      <th>494</th>\n",
              "      <th>495</th>\n",
              "      <th>496</th>\n",
              "      <th>497</th>\n",
              "      <th>498</th>\n",
              "      <th>499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Female</td>\n",
              "      <td>White</td>\n",
              "      <td>28.021737</td>\n",
              "      <td>4.351153</td>\n",
              "      <td>2.453895</td>\n",
              "      <td>1.637143</td>\n",
              "      <td>-1.746628</td>\n",
              "      <td>-0.483463</td>\n",
              "      <td>0.034170</td>\n",
              "      <td>1.399225</td>\n",
              "      <td>-0.795440</td>\n",
              "      <td>0.417474</td>\n",
              "      <td>0.214564</td>\n",
              "      <td>-0.471581</td>\n",
              "      <td>1.945645</td>\n",
              "      <td>-0.676217</td>\n",
              "      <td>1.213878</td>\n",
              "      <td>0.015701</td>\n",
              "      <td>1.472670</td>\n",
              "      <td>-0.054158</td>\n",
              "      <td>0.106858</td>\n",
              "      <td>-1.073194</td>\n",
              "      <td>-1.071848</td>\n",
              "      <td>-0.249942</td>\n",
              "      <td>0.634626</td>\n",
              "      <td>-0.732358</td>\n",
              "      <td>2.445728</td>\n",
              "      <td>0.784284</td>\n",
              "      <td>0.112329</td>\n",
              "      <td>1.055362</td>\n",
              "      <td>-0.605459</td>\n",
              "      <td>1.259140</td>\n",
              "      <td>-0.287927</td>\n",
              "      <td>0.214142</td>\n",
              "      <td>-0.644585</td>\n",
              "      <td>1.165376</td>\n",
              "      <td>-0.409198</td>\n",
              "      <td>-0.705823</td>\n",
              "      <td>0.091147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.655416</td>\n",
              "      <td>-0.012623</td>\n",
              "      <td>0.660826</td>\n",
              "      <td>0.258141</td>\n",
              "      <td>0.036875</td>\n",
              "      <td>-0.229340</td>\n",
              "      <td>0.353817</td>\n",
              "      <td>-0.178814</td>\n",
              "      <td>-0.145229</td>\n",
              "      <td>-0.040692</td>\n",
              "      <td>-0.046980</td>\n",
              "      <td>0.311939</td>\n",
              "      <td>-0.348202</td>\n",
              "      <td>0.271357</td>\n",
              "      <td>0.355443</td>\n",
              "      <td>-0.050447</td>\n",
              "      <td>-0.051816</td>\n",
              "      <td>0.083028</td>\n",
              "      <td>0.184139</td>\n",
              "      <td>0.107824</td>\n",
              "      <td>-0.083415</td>\n",
              "      <td>-0.359288</td>\n",
              "      <td>0.156547</td>\n",
              "      <td>-0.588539</td>\n",
              "      <td>-0.025777</td>\n",
              "      <td>-0.172269</td>\n",
              "      <td>0.331421</td>\n",
              "      <td>0.222768</td>\n",
              "      <td>-0.319124</td>\n",
              "      <td>-0.060476</td>\n",
              "      <td>-0.557444</td>\n",
              "      <td>-0.015627</td>\n",
              "      <td>-0.052749</td>\n",
              "      <td>-0.234189</td>\n",
              "      <td>-0.072384</td>\n",
              "      <td>0.090403</td>\n",
              "      <td>0.376761</td>\n",
              "      <td>0.258914</td>\n",
              "      <td>-0.050558</td>\n",
              "      <td>0.014513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Female</td>\n",
              "      <td>White</td>\n",
              "      <td>29.603342</td>\n",
              "      <td>-3.407193</td>\n",
              "      <td>0.771800</td>\n",
              "      <td>-2.957411</td>\n",
              "      <td>0.599226</td>\n",
              "      <td>-2.805277</td>\n",
              "      <td>0.329414</td>\n",
              "      <td>-2.055339</td>\n",
              "      <td>-1.194446</td>\n",
              "      <td>-0.633159</td>\n",
              "      <td>2.268302</td>\n",
              "      <td>1.159443</td>\n",
              "      <td>0.899266</td>\n",
              "      <td>-0.472739</td>\n",
              "      <td>0.541605</td>\n",
              "      <td>-1.248643</td>\n",
              "      <td>0.046512</td>\n",
              "      <td>1.225688</td>\n",
              "      <td>0.456477</td>\n",
              "      <td>-1.483071</td>\n",
              "      <td>-0.944882</td>\n",
              "      <td>1.483229</td>\n",
              "      <td>0.512809</td>\n",
              "      <td>0.692537</td>\n",
              "      <td>0.178988</td>\n",
              "      <td>-1.609531</td>\n",
              "      <td>-1.985852</td>\n",
              "      <td>-0.469491</td>\n",
              "      <td>-1.156583</td>\n",
              "      <td>0.475535</td>\n",
              "      <td>-0.041015</td>\n",
              "      <td>-0.214832</td>\n",
              "      <td>-0.681641</td>\n",
              "      <td>1.131433</td>\n",
              "      <td>-0.667814</td>\n",
              "      <td>0.267111</td>\n",
              "      <td>-0.112433</td>\n",
              "      <td>...</td>\n",
              "      <td>0.034605</td>\n",
              "      <td>-0.024487</td>\n",
              "      <td>-0.212205</td>\n",
              "      <td>0.440684</td>\n",
              "      <td>-0.065303</td>\n",
              "      <td>0.409878</td>\n",
              "      <td>0.455144</td>\n",
              "      <td>-0.108402</td>\n",
              "      <td>0.027760</td>\n",
              "      <td>-0.015238</td>\n",
              "      <td>0.027453</td>\n",
              "      <td>0.319960</td>\n",
              "      <td>-0.014589</td>\n",
              "      <td>-0.083241</td>\n",
              "      <td>-0.285702</td>\n",
              "      <td>0.047510</td>\n",
              "      <td>-0.144107</td>\n",
              "      <td>0.405289</td>\n",
              "      <td>-0.044139</td>\n",
              "      <td>-0.287215</td>\n",
              "      <td>0.201876</td>\n",
              "      <td>-0.298703</td>\n",
              "      <td>0.347969</td>\n",
              "      <td>0.029646</td>\n",
              "      <td>0.073052</td>\n",
              "      <td>-0.010259</td>\n",
              "      <td>0.023681</td>\n",
              "      <td>0.373202</td>\n",
              "      <td>-0.525402</td>\n",
              "      <td>-0.198727</td>\n",
              "      <td>-0.198440</td>\n",
              "      <td>-0.158843</td>\n",
              "      <td>0.191984</td>\n",
              "      <td>-0.004532</td>\n",
              "      <td>0.229210</td>\n",
              "      <td>-0.173042</td>\n",
              "      <td>-0.072871</td>\n",
              "      <td>0.442939</td>\n",
              "      <td>-0.054423</td>\n",
              "      <td>0.026959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Female</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>26.504283</td>\n",
              "      <td>0.642464</td>\n",
              "      <td>2.522944</td>\n",
              "      <td>-2.197094</td>\n",
              "      <td>2.270646</td>\n",
              "      <td>-0.472510</td>\n",
              "      <td>0.532815</td>\n",
              "      <td>-0.266449</td>\n",
              "      <td>-0.131638</td>\n",
              "      <td>1.038315</td>\n",
              "      <td>-0.865827</td>\n",
              "      <td>-0.811267</td>\n",
              "      <td>-0.381401</td>\n",
              "      <td>-0.801701</td>\n",
              "      <td>-0.485021</td>\n",
              "      <td>0.656005</td>\n",
              "      <td>2.489571</td>\n",
              "      <td>-0.714447</td>\n",
              "      <td>0.658228</td>\n",
              "      <td>-0.075957</td>\n",
              "      <td>-1.159888</td>\n",
              "      <td>-2.334786</td>\n",
              "      <td>-0.253364</td>\n",
              "      <td>-2.073697</td>\n",
              "      <td>-0.939994</td>\n",
              "      <td>-1.177166</td>\n",
              "      <td>0.551689</td>\n",
              "      <td>-1.313316</td>\n",
              "      <td>-0.486217</td>\n",
              "      <td>0.732130</td>\n",
              "      <td>-0.320456</td>\n",
              "      <td>-1.143053</td>\n",
              "      <td>1.297522</td>\n",
              "      <td>-0.617038</td>\n",
              "      <td>0.340978</td>\n",
              "      <td>0.978603</td>\n",
              "      <td>0.398515</td>\n",
              "      <td>...</td>\n",
              "      <td>0.510825</td>\n",
              "      <td>-0.616479</td>\n",
              "      <td>0.644675</td>\n",
              "      <td>0.505319</td>\n",
              "      <td>-0.299894</td>\n",
              "      <td>-0.058435</td>\n",
              "      <td>0.095024</td>\n",
              "      <td>-0.101136</td>\n",
              "      <td>0.042583</td>\n",
              "      <td>0.061005</td>\n",
              "      <td>0.304137</td>\n",
              "      <td>0.259210</td>\n",
              "      <td>-0.022425</td>\n",
              "      <td>0.138097</td>\n",
              "      <td>-0.442536</td>\n",
              "      <td>-0.108350</td>\n",
              "      <td>0.369865</td>\n",
              "      <td>0.151049</td>\n",
              "      <td>0.096285</td>\n",
              "      <td>0.013651</td>\n",
              "      <td>0.175281</td>\n",
              "      <td>0.144344</td>\n",
              "      <td>-0.006250</td>\n",
              "      <td>0.100850</td>\n",
              "      <td>-0.051642</td>\n",
              "      <td>0.122977</td>\n",
              "      <td>-0.088661</td>\n",
              "      <td>-0.229844</td>\n",
              "      <td>-0.272144</td>\n",
              "      <td>0.012633</td>\n",
              "      <td>0.423352</td>\n",
              "      <td>-0.033844</td>\n",
              "      <td>-0.125387</td>\n",
              "      <td>-0.483924</td>\n",
              "      <td>-0.116553</td>\n",
              "      <td>-0.113281</td>\n",
              "      <td>0.015519</td>\n",
              "      <td>0.017111</td>\n",
              "      <td>-0.012309</td>\n",
              "      <td>0.264572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Female</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>25.012088</td>\n",
              "      <td>0.895121</td>\n",
              "      <td>-2.092517</td>\n",
              "      <td>3.687830</td>\n",
              "      <td>0.539642</td>\n",
              "      <td>1.988930</td>\n",
              "      <td>1.121646</td>\n",
              "      <td>2.255337</td>\n",
              "      <td>-0.128801</td>\n",
              "      <td>1.148379</td>\n",
              "      <td>1.616247</td>\n",
              "      <td>-2.599757</td>\n",
              "      <td>-0.322807</td>\n",
              "      <td>2.102508</td>\n",
              "      <td>-0.204551</td>\n",
              "      <td>0.069818</td>\n",
              "      <td>0.745222</td>\n",
              "      <td>-0.859875</td>\n",
              "      <td>-2.235995</td>\n",
              "      <td>-0.207436</td>\n",
              "      <td>-1.678697</td>\n",
              "      <td>-0.569024</td>\n",
              "      <td>-0.723122</td>\n",
              "      <td>-0.144833</td>\n",
              "      <td>-1.537487</td>\n",
              "      <td>1.678429</td>\n",
              "      <td>0.501249</td>\n",
              "      <td>-0.230747</td>\n",
              "      <td>0.746559</td>\n",
              "      <td>-0.069959</td>\n",
              "      <td>-0.346651</td>\n",
              "      <td>0.448291</td>\n",
              "      <td>0.283592</td>\n",
              "      <td>-0.445759</td>\n",
              "      <td>-0.529080</td>\n",
              "      <td>0.287333</td>\n",
              "      <td>0.466766</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.153845</td>\n",
              "      <td>-0.049137</td>\n",
              "      <td>-0.023112</td>\n",
              "      <td>-0.173509</td>\n",
              "      <td>-0.265363</td>\n",
              "      <td>0.091898</td>\n",
              "      <td>0.016743</td>\n",
              "      <td>-0.092894</td>\n",
              "      <td>-0.009915</td>\n",
              "      <td>-0.031731</td>\n",
              "      <td>0.153983</td>\n",
              "      <td>0.001281</td>\n",
              "      <td>0.123019</td>\n",
              "      <td>-0.035719</td>\n",
              "      <td>-0.045633</td>\n",
              "      <td>-0.103204</td>\n",
              "      <td>0.089567</td>\n",
              "      <td>0.104990</td>\n",
              "      <td>0.337228</td>\n",
              "      <td>-0.018783</td>\n",
              "      <td>-0.215437</td>\n",
              "      <td>0.268139</td>\n",
              "      <td>-0.125425</td>\n",
              "      <td>0.095183</td>\n",
              "      <td>-0.125172</td>\n",
              "      <td>-0.226467</td>\n",
              "      <td>0.371647</td>\n",
              "      <td>-0.023041</td>\n",
              "      <td>-0.093040</td>\n",
              "      <td>0.338300</td>\n",
              "      <td>-0.280392</td>\n",
              "      <td>0.046582</td>\n",
              "      <td>0.116709</td>\n",
              "      <td>0.133876</td>\n",
              "      <td>0.072716</td>\n",
              "      <td>0.124083</td>\n",
              "      <td>0.213735</td>\n",
              "      <td>-0.149901</td>\n",
              "      <td>-0.217130</td>\n",
              "      <td>0.004403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Male</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>27.358934</td>\n",
              "      <td>-2.332423</td>\n",
              "      <td>0.154999</td>\n",
              "      <td>-2.623793</td>\n",
              "      <td>1.682456</td>\n",
              "      <td>1.262280</td>\n",
              "      <td>-1.685565</td>\n",
              "      <td>0.489319</td>\n",
              "      <td>-0.043471</td>\n",
              "      <td>-0.372265</td>\n",
              "      <td>1.778535</td>\n",
              "      <td>-1.145419</td>\n",
              "      <td>2.461327</td>\n",
              "      <td>1.396318</td>\n",
              "      <td>-0.911969</td>\n",
              "      <td>-2.228570</td>\n",
              "      <td>1.378633</td>\n",
              "      <td>-1.512325</td>\n",
              "      <td>-0.440331</td>\n",
              "      <td>-0.111163</td>\n",
              "      <td>-0.885884</td>\n",
              "      <td>-0.840501</td>\n",
              "      <td>1.576620</td>\n",
              "      <td>-0.972075</td>\n",
              "      <td>-2.008346</td>\n",
              "      <td>-0.358732</td>\n",
              "      <td>0.896535</td>\n",
              "      <td>0.562193</td>\n",
              "      <td>0.154542</td>\n",
              "      <td>-1.077315</td>\n",
              "      <td>1.902062</td>\n",
              "      <td>1.728109</td>\n",
              "      <td>0.317205</td>\n",
              "      <td>-0.436143</td>\n",
              "      <td>0.226549</td>\n",
              "      <td>-0.502206</td>\n",
              "      <td>-0.157102</td>\n",
              "      <td>...</td>\n",
              "      <td>0.181831</td>\n",
              "      <td>-0.026589</td>\n",
              "      <td>-0.051453</td>\n",
              "      <td>0.261819</td>\n",
              "      <td>-0.048644</td>\n",
              "      <td>-0.099526</td>\n",
              "      <td>-0.026777</td>\n",
              "      <td>0.039836</td>\n",
              "      <td>-0.168277</td>\n",
              "      <td>0.077232</td>\n",
              "      <td>0.193722</td>\n",
              "      <td>0.093298</td>\n",
              "      <td>-0.075132</td>\n",
              "      <td>-0.063202</td>\n",
              "      <td>0.120167</td>\n",
              "      <td>0.039270</td>\n",
              "      <td>0.350429</td>\n",
              "      <td>0.166559</td>\n",
              "      <td>0.130134</td>\n",
              "      <td>-0.181019</td>\n",
              "      <td>-0.193276</td>\n",
              "      <td>0.312204</td>\n",
              "      <td>-0.187331</td>\n",
              "      <td>-0.029194</td>\n",
              "      <td>-0.212277</td>\n",
              "      <td>-0.463872</td>\n",
              "      <td>0.041810</td>\n",
              "      <td>0.041185</td>\n",
              "      <td>-0.182479</td>\n",
              "      <td>-0.182461</td>\n",
              "      <td>-0.019350</td>\n",
              "      <td>-0.093371</td>\n",
              "      <td>0.003443</td>\n",
              "      <td>-0.025467</td>\n",
              "      <td>0.155397</td>\n",
              "      <td>-0.067609</td>\n",
              "      <td>-0.084833</td>\n",
              "      <td>0.033429</td>\n",
              "      <td>-0.199198</td>\n",
              "      <td>0.229629</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 503 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label  Gender Ethnicity          0  ...       496       497       498       499\n",
              "0      0  Female     White  28.021737  ...  0.376761  0.258914 -0.050558  0.014513\n",
              "1      0  Female     White  29.603342  ... -0.072871  0.442939 -0.054423  0.026959\n",
              "2      1  Female  Hispanic  26.504283  ...  0.015519  0.017111 -0.012309  0.264572\n",
              "3      0  Female  Hispanic  25.012088  ...  0.213735 -0.149901 -0.217130  0.004403\n",
              "4      1    Male  Hispanic  27.358934  ... -0.084833  0.033429 -0.199198  0.229629\n",
              "\n",
              "[5 rows x 503 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo4na9kCw6bN"
      },
      "source": [
        "# remove all nans --> we use the variable \"data\" in the rest of this notebook\n",
        "data = raw_data.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLwniYoCY6_y"
      },
      "source": [
        "def split_data_from_df(data):\n",
        "  y = data['Label'].values\n",
        "  # g = data['Gender'].values\n",
        "  # e = data['Ethnicity'].values\n",
        "  X = data[np.arange(500)].values\n",
        "  filter_col = ['Ethnicity','Gender'] + [col for col in data if str(col).startswith('Ethnicity_')] + [col for col in data if str(col).startswith('Gender_')] \n",
        "  dem = data[filter_col].copy()\n",
        "  return X,y,dem\n",
        "def encode(df):\n",
        "  g_enc = LabelEncoder()\n",
        "  e_enc = LabelEncoder()\n",
        "  df['Gender'] = g_enc.fit_transform(df['Gender'])\n",
        "  df['Ethnicity'] = e_enc.fit_transform(df['Ethnicity'])\n",
        "  return df, g_enc,e_enc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzvvNPjWL5_-"
      },
      "source": [
        "## **1. Install and import aif360**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPTo6bhBI52X",
        "outputId": "ebc105af-c4fd-469b-e6d0-b7c29173780c"
      },
      "source": [
        "!pip install aif360 #install aif360"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aif360 in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.1.5)\n",
            "Requirement already satisfied: tempeh in /usr/local/lib/python3.7/dist-packages (from aif360) (0.1.12)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from aif360) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aif360) (3.2.2)\n",
            "Requirement already satisfied: scipy<1.6.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360) (1.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (2.4.7)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (3.6.4)\n",
            "Requirement already satisfied: memory-profiler in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (0.58.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (2.23.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (0.39.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler->tempeh->aif360) (5.4.8)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (21.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (57.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (8.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (2021.5.30)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (1.3.0)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (0.0.7)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (4.62.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (0.51.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap->tempeh->aif360) (0.34.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqLHs_GJJBeF"
      },
      "source": [
        "import aif360\n",
        "from aif360.metrics import ClassificationMetric\n",
        "from aif360.datasets import BinaryLabelDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6hoB7SDMGnm"
      },
      "source": [
        "## **2. Create BinaryLabelDatasets objects**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MZWmsLr_AIk"
      },
      "source": [
        "We first need to process our data so it is in the standard aif360 format. aif360 uses custom dataset classes. We will use the BinaryLabelDataset for our problem. The BinaryLabelDataset has four components:\n",
        "- Features : features used to train the model\n",
        "- Labels : Outcome of binary classification\n",
        "- Scores: Probability of outcome (not available in our case)\n",
        "- Protected characteristics (*Gender* and *Ethnicity* columns for us)\n",
        "\n",
        "In the code below we define the model and split our data into train and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LZ1u1fVLZRY",
        "outputId": "17bebef8-b005-45d7-97fa-1c17365e03e0"
      },
      "source": [
        "model = RidgeClassifier(random_state = 42)\n",
        "# split into train/test\n",
        "data_train, data_test = train_test_split(data,test_size = 0.3,random_state=4)\n",
        "# # drop unecessary columns\n",
        "# cols_to_keep = ['Label','Gender','Ethnicity'] + list(np.arange(500))\n",
        "# data_train = data_train[cols_to_keep].copy()\n",
        "# data_test = data_test[cols_to_keep].copy()\n",
        "# make Gender and Ethnicity numerical\n",
        "data_train,g_enc,e_enc = encode(data_train.copy())\n",
        "data_test,_,_ = encode(data_test.copy())\n",
        "print(\"Gender codes - Male : %d - Female : %d\"%(g_enc.transform(['Male'])[0],g_enc.transform(['Female'])[0]))\n",
        "print(\"Ethnicity codes - White : %d - Black : %d - Asian : %d - Hispanic : %d\"%(e_enc.transform(['White'])[0],e_enc.transform(['Black'])[0],e_enc.transform(['Asian'])[0],e_enc.transform(['Hispanic'])[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gender codes - Male : 1 - Female : 0\n",
            "Ethnicity codes - White : 3 - Black : 1 - Asian : 0 - Hispanic : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGB67pe8piG2"
      },
      "source": [
        "**Questions** : \n",
        "- **Make two BinaryLabelDatasets: one for data_train and one for data_test.** See [documentation here](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.BinaryLabelDataset.html#aif360.datasets.BinaryLabelDataset) for BinaryLabelDataset and [here](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.StructuredDataset.html#aif360.datasets.StructuredDataset) for StructuredDataset the base class on which the former is built. You will need to specify the following arguments: *favorable_label, unfavorable_label,df,label_names,protected_attribute_names*.\n",
        "- **Update the features so they don't contain the protected attributes.** By default, the protected attributes are used as features by the BinaryLabelDataset object. You can check which columns are included using *ds.feature_names*. You can then specify which features you want to use instead using *ds.feature_names = np.arange(500).astype(str)*. Here we set the features to be the 500 pre-defined features, whose columns names are 0,1..499."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEDoHGF8NVt7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV7HUVoiKE0T"
      },
      "source": [
        "## **3. Train the model and get the predictions for the test set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFoOaE65KE7r"
      },
      "source": [
        "- **Train the model and get the predictions for the test set.** You can use *X_train=ds_train.features* and *y_train=ds_train.labels.ravel()* to access X and y from the dataset you created earlier; and similarly for *X_test,y_test*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qipZ8AjwNaG_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq0-K1xSKaNy"
      },
      "source": [
        "## **4. Create a ClassificationMetric object and calculate accuracy and faitness metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcIy-8AqpiG2"
      },
      "source": [
        "Now we will need to choose what privileged group and unprivileged group we want to focus on. We will demonstrate for *Black* vs *White*. We define our privileged group and unprivileged group as such:\n",
        "\n",
        "```python\n",
        "    unprivileged_groups = [{'Ethnicity':1}]  #1 is the code for 'Black'\n",
        "    privileged_groups = [{'Ethnicity':3}]   #3 is the code for 'White \n",
        "```\n",
        "The codes for each gender/ethnicity are given just after where we define the model and train/test set.\n",
        "           \n",
        "**Questions** : \n",
        "- **Create a ClassificationMetric object for the test set**. See [doc](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html). It takes 4 arguments: \n",
        "    1. dataset: ds_test = the test dataset you created above \n",
        "    2. classified_dataset: ds_test_pred = the same dataset but replacing the labels with the predictions as such:\n",
        "    ```python\n",
        "            ds_test_pred = ds_test.copy()\n",
        "            ds_test_pred.labels = y_pred_test\n",
        "    ```\n",
        "    3. unprivileged_groups\n",
        "    4. privileged_groups\n",
        "    \n",
        "- **Calculate the Accuracy**\n",
        "- **Claculate the Statistical Parity Difference, Disparate Impact, Equal Opportunity Difference.** \n",
        " The available metrics are [here](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html).\n",
        " \n",
        " \n",
        "Note: if you specify more than one privileged/unprivileged groups, aif360 aggregates them as if they were one privileged/unprivileged group and compute the fairness metrics for the aggregate. Hence to compute for instance the fairness metrics for the *Asian* group, you should create a new ClassificationMetric object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuLUxf1-NeOT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FHutlbRNeRP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DENBCL2AMc3I"
      },
      "source": [
        "## **Final remarks**\n",
        "**You should find the same results as in the previous section**. \n",
        "\n",
        "Note that for each analysis, we only obtained one number for fairness metrics. As mentioned before, in a real life setting, you should calculate a confidence interval by repeating the experiments for different train/test splits. A way to do this is for instance to use a non-parametric bootstrap method as described in section 10.2 from the book [Computer Age Statistical Inference by B.Efron and T.Hastie](https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf) that we linked earlier. We give an example of this in the next and last notebook. Note that there will be nothing for you to do in this last notebook. "
      ]
    }
  ]
}