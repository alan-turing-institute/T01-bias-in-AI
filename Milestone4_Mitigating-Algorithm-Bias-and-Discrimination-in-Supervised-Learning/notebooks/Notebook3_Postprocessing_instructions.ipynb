{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzgpJmDOSDfR"
      },
      "source": [
        "# **Post-processing technique: Equalised Odds Post-processing**\n",
        "\n",
        "The Equalised Odds Post-Processing approach (Hardt et al 2016) flips predictions at random until a desired error rate distribution between the protected group and the rest of the sample is achieved. The steps we will take are outlined below.\n",
        "\n",
        "1. First, we will calculate Disparate Impact and Statistical Parity Difference metrics for a baseline model with no fairness intervention.\n",
        "2. We will then apply the Equalised Odds Post-processing method to data used to train a predictive model and observe the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaV6sRojLqem"
      },
      "source": [
        "# Install Libraries and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwY66NktyspI"
      },
      "outputs": [],
      "source": [
        "# install holisticai\n",
        "!pip install holisticai\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from holisticai.bias.mitigation import EqualizedOdds\n",
        "from holisticai.bias import metrics as bias_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NnCNsoTLdHX"
      },
      "source": [
        "Load the data into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RoDxmC_wh9f"
      },
      "outputs": [],
      "source": [
        "# suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "# Load data\n",
        "from sklearn.datasets import fetch_openml\n",
        "bunch = fetch_openml(data_id=44270)\n",
        "df = bunch['frame'].dropna()\n",
        "df['Ethnicity_White'] = (df['Ethnicity'] == 'White')*1\n",
        "df['Ethnicity_Black'] = (df['Ethnicity'] == 'Black')*1\n",
        "df = df.drop(columns = ['Gender', 'Ethnicity'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9kLf_pUSuCp"
      },
      "source": [
        "# Run a baseline predictive model without applying post-processing\n",
        "First we will build a Logistic Regression classifier and observe some baseline results, using the original data without Post-processing.\n",
        "\n",
        "Set up variables for the privileged and unprivileged groups. In this example we will assign 'Ethnicity_White' as our privileged group and Black as the unpriviledged group.\n",
        "\n",
        "Train a Logistic Regression model with 10 fold stratified cross validation. Compute performance metrics (Accuracy, Precision, Recall and F1 Score) and fairness metrics (Equalized Odds Difference, False Negative Rate Difference, Statistical Parity Difference).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ2SdI2ozfsM"
      },
      "outputs": [],
      "source": [
        "# Instantiate the classifier (this code is ready to run, there are no gaps to fill)\n",
        "model = LogisticRegression(random_state=10, solver=\"lbfgs\", penalty=\"none\")\n",
        "\n",
        "# instantiate the cross-validation scheme\n",
        "mv = StratifiedKFold(n_splits=10, shuffle=True, random_state=10)\n",
        "\n",
        "# setup the performance metrics to be computed\n",
        "perf_metrics = {\"Accuracy\": metrics.accuracy_score, \n",
        "                \"Precision\": metrics.precision_score, \n",
        "                \"Recall\": metrics.recall_score,\n",
        "                \"F1-Score\": metrics.f1_score, \n",
        "                }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDDMnHRmPtZk"
      },
      "outputs": [],
      "source": [
        "# Train a logistic regression classifier on the dataset (this code is ready to run, there are no gaps to fill)\n",
        "k, i = True, 1\n",
        "\n",
        "# instantiating X\n",
        "X = df.drop(columns=[\"Label\"])\n",
        "\n",
        "# instantiating the target variable\n",
        "y = df['Label']\n",
        "\n",
        "for (train, test) in mv.split(X, y):\n",
        "\n",
        "    # instantiating X\n",
        "    X_train = X.iloc[train].copy()\n",
        "\n",
        "    # instantiating y\n",
        "    y_train = y.iloc[train].copy()\n",
        "\n",
        "    # fit model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # X_test \n",
        "    X_test = X.iloc[test]\n",
        "    \n",
        "    # set up vectors\n",
        "    group_a = X_test.Ethnicity_Black == 1\n",
        "    group_b = X_test.Ethnicity_White == 1\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_true = y.iloc[test].values.ravel()\n",
        "    params = [group_a, group_b, y_pred]\n",
        "\n",
        "    # compute performance metrics\n",
        "    metric_list = []\n",
        "    for pf in perf_metrics.keys():\n",
        "            metric_list += [[pf, perf_metrics[pf](y_true, y_pred)]]\n",
        "    \n",
        "    # Compute fairness metrics\n",
        "    metric_list += [['Statistical Parity Difference', bias_metrics.statistical_parity(group_a, group_b, y_pred)]]\n",
        "    metric_list += [['Disparate Impact', bias_metrics.disparate_impact(group_a, group_b, y_pred)]]\n",
        "    metric_list += [['Equalized Odds Difference', bias_metrics.average_odds_diff(group_a, group_b, y_pred,y_true)]]\n",
        "    metric_list += [['False Negative Rate Difference', bias_metrics.false_negative_rate_diff(group_a, group_b, y_pred,y_true)]]\n",
        "\n",
        "    # concatenate results\n",
        "    df_m = pd.DataFrame(metric_list, columns=[\"Metric\", \"Value\"])\n",
        "    df_m[\"Fold\"] = i\n",
        "    i += 1\n",
        "    if k:\n",
        "        df_metrics_orig = df_m.copy()\n",
        "        k=0\n",
        "    else:\n",
        "        df_metrics_orig = pd.concat([df_metrics_orig, df_m.copy()], axis=0, ignore_index=True)\n",
        "\n",
        "df_metrics_orig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "WykgeYwFQZGd",
        "outputId": "132b17b3-8247-4369-805a-3a75265a3b58"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Value</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Accuracy</th>\n",
              "      <td>0.712141</td>\n",
              "      <td>0.008402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equalized Odds Difference</th>\n",
              "      <td>-0.058946</td>\n",
              "      <td>0.012744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F1-Score</th>\n",
              "      <td>0.593431</td>\n",
              "      <td>0.011609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>False Negative Rate Difference</th>\n",
              "      <td>0.072792</td>\n",
              "      <td>0.023473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Precision</th>\n",
              "      <td>0.648655</td>\n",
              "      <td>0.013874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Recall</th>\n",
              "      <td>0.546962</td>\n",
              "      <td>0.012234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Statistical Parity Difference</th>\n",
              "      <td>-0.070907</td>\n",
              "      <td>0.010773</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    mean       std\n",
              "                                   Value     Value\n",
              "Metric                                            \n",
              "Accuracy                        0.712141  0.008402\n",
              "Equalized Odds Difference      -0.058946  0.012744\n",
              "F1-Score                        0.593431  0.011609\n",
              "False Negative Rate Difference  0.072792  0.023473\n",
              "Precision                       0.648655  0.013874\n",
              "Recall                          0.546962  0.012234\n",
              "Statistical Parity Difference  -0.070907  0.010773"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: Display metrics\n",
        "\n",
        "# /TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJOxrqbNvto2"
      },
      "source": [
        "# Apply Equalised Odds Post-processing to the predictive model\n",
        "\n",
        "Amend your Logistic Regression routine above to apply [Equalized Odds](https://holisticai.readthedocs.io/en/latest/generated/holisticai.bias.mitigation.EqualizedOdds.html#holisticai.bias.mitigation.EqualizedOdds) Post-Processing to each fold of training data.  Compute performance metrics (Accuracy, Precision, Recall and F1 Score) and fairness metrics (Equalized Odds Difference, False Negative Rate Difference, Statistical Parity Difference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "001wC_i8N0M-"
      },
      "outputs": [],
      "source": [
        "\n",
        "k, i = True, 1\n",
        "\n",
        "# instantiating X\n",
        "X = df.drop(columns=[\"Label\"])\n",
        "\n",
        "# instantiating the target variable\n",
        "y = df['Label']\n",
        "\n",
        "for (train, test) in mv.split(X, y):\n",
        "\n",
        "    # Set up the data\n",
        "    X_train = X.iloc[train].copy()\n",
        "    y_train = y.iloc[train].copy()\n",
        "\n",
        "    # fit model\n",
        "    model = model.fit(X_train, y_train)\n",
        "    \n",
        "    # get predictions in the test set\n",
        "    X_test = X.iloc[test].copy()\n",
        "    y_test = y.iloc[test].copy()\n",
        "    ypred_prob = model.predict_proba(X_test).ravel()[1::2] # get probabilities\n",
        "    ypred_class = model.predict(X_test)\n",
        "\n",
        "    # fit post-processing using results from 60% of the test set\n",
        "    test_pct = 0.4\n",
        "    n = int(len(y_test))\n",
        "    n_2 = int(n* (1-test_pct))\n",
        "    indices = np.random.permutation(n)\n",
        "    pp_indices = indices[:n_2]\n",
        "    test_indices = indices[n_2:]\n",
        "\n",
        "    # set up data\n",
        "    group_a = X_test.reset_index().Ethnicity_Black == 1\n",
        "    group_b = X_test.reset_index().Ethnicity_White == 1\n",
        "    y_train_pp = np.array(y_test)[pp_indices]\n",
        "    y_pred_train = np.array(ypred_class)[pp_indices]\n",
        "    group_a_train = np.array(group_a)[pp_indices]\n",
        "    group_b_train = np.array(group_b)[pp_indices]\n",
        "    y_pred_test = np.array(ypred_class)[test_indices]\n",
        "    group_a_test = group_a[test_indices]\n",
        "    group_b_test = group_b[test_indices]\n",
        "    y_true_test = np.array(y_test)[test_indices]\n",
        "\n",
        "    # TODO Use eq to post-process predictions on the other 40% of the test set\n",
        "\n",
        "    # TODO fit it\n",
        "\n",
        "    # TODO transform\n",
        "\n",
        "    # TODO get new labels\n",
        "\n",
        "\n",
        "    # compute performance metrics\n",
        "    metric_list = []\n",
        "    for pf in perf_metrics.keys():\n",
        "            metric_list += [[pf, perf_metrics[pf](y.iloc[test].values.ravel(), ypred_class.ravel())]]\n",
        "    \n",
        "    # Compute fairness metrics\n",
        "    metric_list += [['Statistical Parity Difference', bias_metrics.statistical_parity(group_a_test, group_b_test, y_pred)]]\n",
        "    metric_list += [['Disparate Impact', bias_metrics.disparate_impact(group_a_test, group_b_test, y_pred)]]\n",
        "    metric_list += [['Equalized Odds Difference', bias_metrics.average_odds_diff(group_a_test, group_b_test, y_pred ,y_true_test)]]\n",
        "    metric_list += [['False Negative Rate Difference', bias_metrics.false_negative_rate_diff(group_a_test, group_b_test, y_pred ,y_true_test)]]\n",
        "\n",
        "    # concatenate results\n",
        "    df_m = pd.DataFrame(metric_list, columns=[\"Metric\", \"Value\"])\n",
        "    df_m[\"Fold\"] = i\n",
        "    i += 1\n",
        "    if k:\n",
        "        df_metrics = df_m.copy()\n",
        "        k=0\n",
        "    else:\n",
        "        df_metrics = pd.concat([df_metrics, df_m.copy()], axis=0, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "hIq9sP3FO8lO",
        "outputId": "465d8df0-8359-4f20-c208-1ea6e38326a4"
      },
      "outputs": [],
      "source": [
        "# TODO: Display metrics\n",
        "\n",
        "# /TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuYBVBoywMyh"
      },
      "source": [
        "# Present results to show the effectiveness of the Post-processing method\n",
        "\n",
        "Present graphs (bar charts work well) to show how each performance and fairness metric differs for the baseline model compared with the application of Post-processing. Show the target line for each metric on the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FApym7uMzeha",
        "outputId": "6259cd80-e11d-406b-8895-e83c8aa93a64"
      },
      "outputs": [],
      "source": [
        "# TODO: Present graphs to show each performance and fairness metrics\n",
        "\n",
        "# /TODO\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
