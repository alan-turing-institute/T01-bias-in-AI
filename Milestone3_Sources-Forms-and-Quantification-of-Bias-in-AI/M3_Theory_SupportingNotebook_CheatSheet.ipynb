{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCLYAEZ_zw33"
   },
   "source": [
    "# Milestone 3: Sources, Forms, and Quantification of Bias and Discrimination in Supervised Learning\n",
    "# PART I - THEORY \n",
    "\n",
    "# 1 - Introduction\n",
    "\n",
    "In the previous milsetones, Fairness was explored in its ethical, legal and governance aspect. \n",
    "\n",
    "In this part of the course, we focus on how we deal with fairness in practice in Supervised Learning. Where does the discrimination comes from? What are the different ways for a model to be unfair?  How can we measure discrimination?\n",
    "\n",
    "\n",
    "This milestone is divided in two parts: <br>\n",
    "- **Part I** presents some theoretical aspects and provides some external references. It is itself divided in 3 parts: \n",
    "    - Real world examples of biased algorithms\n",
    "    - Sources of bias\n",
    "    - Definitions of fairness and associated quantification of bias <br>\n",
    "- **Part II** consists of an exercise notebook which explores a specific example (hiring algorithm). \n",
    "\n",
    "We call protected attributes the features of an individual such as race, gender, age which we aim the model to be fair towards. We restrain ourselves to binary problems, where the outcome is either 0 (negative) or 1 (positive). The example throughout Milestone 3 and 4 will be a hiring algorithm where individuals with a positive outcome go to the next step. \n",
    "\n",
    "This notebook is complementary to the video and slides and covers the same material, with some additional references linked.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9umMqCXH7kF"
   },
   "source": [
    "# 2 - Bias in algorithm: real world examples\n",
    "In the video and slides, we discuss a few real-life examples of bias in algorithms. Below are some references regarding these famous examples. <br> <br>\n",
    "- **COMPAS**.\n",
    "    - https://www.uclalawreview.org/injustice-ex-machina-predictive-algorithms-in-criminal-sentencing/\n",
    "    - https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
    "- **Amazon's recruiting**\n",
    "    - https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10?r=US&IR=T\n",
    "    - https://www.aclu.org/blog/womens-rights/womens-rights-workplace/why-amazons-automated-hiring-tool-discriminated-against\n",
    "- **Apple's credit**\n",
    "    - https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/\n",
    "- **Gender shades**\n",
    "    - http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf\n",
    "    - http://gendershades.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTUZ6bGcH7kH"
   },
   "source": [
    "# 3 - Sources of Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdTZGLU9H7kI"
   },
   "source": [
    "One can refer to the two following papers for details on the different possible bias encountered:\n",
    "- (Mehrabi et al., 2021) : [A survey on bias and fairness in machine learning](https://arxiv.org/pdf/1908.09635.pdf)\n",
    "- (Suresh & Guttag, 2019) : [A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle](https://arxiv.org/pdf/1901.10002.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QO29-qP1H7kK"
   },
   "source": [
    "We describe here a few common bias that follow Suresh & Guttag classification. Those are detailed and presented in the video/slides. \n",
    "- **Historical bias**: Pre-existing bias reflected in the data.\n",
    "- **Representation bias**: When certain parts of the input space are underrepresented (sampling methods, training data)\n",
    "- **Measurement bias**: Measurement process/ data quality may vary across groups (e.g. arrest rate as a measure of crime rate)\n",
    "- **Aggregation bias**: Difference across groups might require several models rather than a one-size fit all model.\n",
    "- **Evaluation bias**: When testing on benchmarks that are unbalanced compared to target population (e.g. facial recognition)\n",
    "- **Deployment bias**: Mismatch between design purpose and use (e.g. person’s likelihood of committing a future crime used for determining the length of a sentence).\n",
    "\n",
    "The following Figure taken from (Suresh & Guttag, 2019) illustrates how those bias interfere with the entire life-cycle of an algortithm and its use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cheatsheet_img1.png\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "\n",
    "\n",
    "*Taken from*: Suresh, H., & Guttag, J. V. (2019). A framework for understanding unintended consequences of machine learning. arXiv preprint arXiv:1901.10002."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Multitude of fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now give an overview of what constitutes fairness in the Fair-ML literature.  \n",
    "\n",
    "There are many mathematical definition of fairness in the literature, and it is commonly accepted that it is not possible to satisfy them all at the same time. Choosing a definition must be done depending on context and application, and we will comment on this later. In [Fairness Definitions Explained](http://www.ece.ubc.ca/~mjulia/publications/Fairness_Definitions_Explained_2018.pdf) from 2018, S.Verma and J.Rubin review the main definitions of fairness found in the literature in recent years (up to 2018) for classification problems. The paper [A Survey on Bias and Fairness in Machine Learning](https://arxiv.org/pdf/1908.09635.pdf)  by Mehrabi et al. also present a number of definition. Broadly speaking, we can split fairness as either individual fairness or group fairness:\n",
    "\n",
    "*   **Individual fairness** : seeks for similar individuals to be treated similarly. \n",
    "*   **Group fairness** : split a population into groups defined by protected attributes and seeks for some measure to be equal across groups.\n",
    "\n",
    "\n",
    "Let's explicit a few definitions belonging to each of these category in the context of a binary classification problem (with outcome being 0-negative or 1-positive). Note that the names of the concepts are not universal and different papers might use different names. The names we use can be found in the two papers linked above, as well as in the Holistic AI's library: https://www.holisticai.com/open-source. Holisticai is a python Fair-ML package that allows to quantify and mitigate bias. We will use it in Part 2 of this milestone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Individual fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness through Unawareness\n",
    "- **Definition:** \"An algorithm is fair as long as any protected attributes A are not explicitly used in the decision-making process\" (Mehrabi et al.,2021)\n",
    "- **Mathematically**: There is no other requirements than the absence of explicit protected attributes from the input features.\n",
    "- **Challenges/Uses in practice**: The presence of proxies in the data make this method often insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness through Awareness (a.k.a. individual fairness)\n",
    "(Dwork et al., 2011) [Fairness Through Awareness](https://arxiv.org/pdf/1104.3913.pdf)\n",
    "- **Definition:** \"An algorithm is fair if it gives similar predictions to similar individuals\" (Mehrabi et al.,2021)\n",
    "- **Mathematically**:Two individuals $x, y$ are at distance $d(x, y) ∈ [0, 1]$. We consider a randomized classifier that maps individuals to probability distributions over outcomes (i.e., the individual $x$ will be classified as $0$ with a probability $p_x$ and as $1$ with a probability $1-x$). This definition of fairness imposes a Lipschitz condition that states that the statistical distance $D(M(x),M(y))$ between $M(x)$ and $M(y)$ must be at most $d(x, y)$ for all pairs of individuals $(x,y)$.\n",
    "- **Challenges/Uses in practice**: How can one choose a distance/similarity metric for individuals in the first place? (Dwork et al., 2011) states that: \"The similarity metric expresses ground truth. When ground truth is unavailable, the metric may reflect the “best” available approximation as agreed upon by society.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Counterfactual fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Kusner et al.,2017) [Counterfactual Fairness](https://arxiv.org/pdf/1703.06856.pdf)\n",
    "\n",
    "* **Definition**: Counterfactual fairness assumes that a causal graph between features and the predited output is known. The causal graph is counterfactuall fair if the predicted outcome does not depend on a descendant of the protected attribute. The intuition is that the decision would be the same in a counterfactual world where the individual belongs to a different demographic group, i.e., if we change the protected attribute whilst keeping the factors that are not causally dependent on it constant, this should not change the distribution of the predicted outcome.  \n",
    "\n",
    "* **Mathematically**: Let's assume we have a causal model such as the ones represented by the below directed acyclic graph (DAG) (taken from Kusner et al., 2017):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cheatsheet_img2.png\" style=\"margin-left:auto; margin-right:auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with: <br>\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "  \\begin{aligned}\n",
    "    A = \\text{protected attributes}\\\\\n",
    "    X = \\text{remaining attributes} \\\\\n",
    "    Y = \\text{predicted output} \\\\\n",
    "    V = A \\cup X = \\text{observable variables} \\\\\n",
    "    U = \\text{latent background variables not caused by V} \\\\\n",
    "  \\end{aligned}\n",
    "\\right. $$\n",
    "<br>\n",
    "Predictor $\\hat{Y}$ is counterfactually fair if under any context $X = x$ and $A = a$:\n",
    "<center>$\n",
    "P(\\hat{Y}_{A←a} (U) = y | X = x, A = a) = P(\\hat{Y}_{A←a'} (U) = y | X = x, A = a),\n",
    "$ </center> <br>\n",
    "for all $y$ and for any value $a'$ attainable by $A$.\n",
    "\n",
    "* **Illustration**: We take here directly the example given in (Kusner et al.,2017) which can be represented using the DAG on the right from above. A car insurance company predicts the accident rate of individuals $Y$. We have:\n",
    "    - $U$ = unobserved variable = aggressive driving\n",
    "    - $A$ = protected attribute = race\n",
    "    - $X$ = remaining attribute = preference for red cars\n",
    "    - $U -> X,Y$ : $U$ causes (a) a higher likelhood of accident and (b) a preference for red cars, hence impacts both $X$ and $Y$. \n",
    "    - $A -> X$: Being of a certain race $A$ is also linked with a preference for red cars, but these individuals are not more likely to be aggressive drivers or involved in accidents than others\n",
    "    \n",
    "    This predictor would be counterfactually fair if $Y$ did not depend on a descendant of $A$, or in other words, if changing $A$ while holding $U$ did not impact $Y$. Here it is impacted as changing $A$ changes $X$ which in turns changes $Y$, hence it is not counterfactually fair. Intuitively, this makes sense as using the preference for red car as a measure of the accident rate will penalize the individuals of a certain race that prefer red cars. \n",
    "\n",
    "* **Challenges/Uses in practice**: Building a causal graph is not easy and can become very unpractical with a high number of features. Additionally, a correlation between features does not always imply causation and if it does, in what direction? If it doesn't, would the machine learning model not use that feature as if it did ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Group fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the two following definitions, let's introduce an example with its confusion matrices. Let's take the example of a math exam with either pass or fail result. There are 100 candidates in total: 50 male and 50 female. We split the candidates in male and female candidates and obtain the following confusion matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/cheatsheet_img3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6QLSWjcfCB-"
   },
   "source": [
    "For the metrics in this part, one can refer to the Holistic AI's [documentation](https://holisticai.readthedocs.io/en/latest/metrics.html) and [library](https://github.com/holistic-ai/holisticai)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUBClFuTH7kX"
   },
   "source": [
    "### Equality of Outcome : Demographic parity\n",
    "- **Definition:** When the likelihood of positive outcome is equal for individuals regardless of whether they are in the protected group or not.\n",
    "- **Mathematically**: If $A$ is the protected attribute, we want $P(\\hat{Y}=1|A=0) = P(\\hat{Y}=1|A=1)$ irrespective of the ground-truth label. <br>\n",
    "- **Specific Metrics**: Let's call these probability Success Rates and note them $SR_0$ (success rate of the unprivileged group) and $SR_1$ (privileged group). There are different metrics for this same concept, we present two common ones:\n",
    "     - **Statistical parity** measures the difference in success rates: $SP= SR_0 - SR_1$. The ideal value is 0.\n",
    "     - **Disparate impact** measures the ratio: $DI = \\frac{SR_0}{SR_1}$. The ideal value is 1. Fair values are usually considered to be between 0.8 ans 1.2.\n",
    "- **Illustration**: In the above example, there are 37 Male candidates predicted to pass against 26 Female candidates out of 50 for each. We have $SR_m= 0.74$ and $SR_f=0.52$, which gives: <br>\n",
    "<center>$\n",
    "\\left\\{ \n",
    "  \\begin{aligned}\n",
    "    SP = 0.52-0.74 = -0.22\\\\\n",
    "    DI = 0.52/0.74 = 0.7 \n",
    "  \\end{aligned}\n",
    "\\right. $ </center>\n",
    "\n",
    "- **Challenges/Uses in practice**: In practice, there might be historical (or other) biases which leads to an uneven success rate across different groups (e.g. more white males as CEO than dark-skinned or female individuals). The problem might be more a societal one than on of the algorithm, and solving it within the algorithm remit might means introducing positive discrimination, which asks sensitive societal and legal questions. For instance in hiring, this migh lead to \"more qualified\" privileged candidates being refused opportunities to the advantage of \"less qualified\" unprivileged candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5XuCmqKH7kY"
   },
   "source": [
    "###  Equality of Opportunity :  Equalised Odds\n",
    "- **Definition:** The probability of a person in the positive class being correctly assigned a positive outcome and the probability of a person in a negative class being incorrectly assigned a positive outcome should both be the same for the privileged and unprivileged group members (for instance male and female).\n",
    "- **Mathematically**: (Mehrabi et al.,2021): \"A predictor $\\hat{Y}$ satisfies equalized odds with respect to protected attribute $A$ and outcome $Y$, if $\\hat{Y}$ and $A$ are independent conditional on $Y$. <center>$\n",
    "P(\\hat{Y}=1|A=0,Y =y) = P(\\hat{Y}=1|A=1,Y =y) , y ∈\\{0,1\\} \"\n",
    "$ </center> <br>\n",
    "Note that: \n",
    "    - $P(\\hat{Y}=1|A=a,Y=0)$ can be estimated using the False Positive Rate (FPR) in a specific problem, i.e. the proportion of positive prediction for individuals that actually have a negative outcome. We have $FPR = \\frac{FP}{FP+TN}$.\n",
    "    - $P(\\hat{Y}=1|A=a,Y=1)$ can be estimated using the True Positive Rate (TPR), i.e. the proportion of positive prediction for individuals that actually have a positive outcome. We have $TPR = \\frac{TP}{FN+TP}$.\n",
    "- **Specific Metrics**: There are different metrics for this same concept, we present two common ones:\n",
    "     - **Average Odds Difference**: $AOD = \\frac{1}{2} [(FPR_{0} - FPR_{1})+ (TPR_{0} - TPR_{1})]$\n",
    "     - **Equal Opportunity Difference**: $EOD = TPR_{0}- TPR_{1}$ <br>\n",
    "    where 0 represents the unprivileged group and 1 the privileged one. For both metrics, the ideal value is 0 and fair values are considered between -0.1 and 0.1.\n",
    "- **Illustration**: In the above example, we have $FPR_{male} = \\frac{FP}{FP+TN}= \\frac{7}{17} = 0.41$; $TPR_{male} = \\frac{TP}{FN+TP}= \\frac{30}{33} = 0.91$;   $FPR_{female}= \\frac{4}{22} = 0.18$ ,$TPR_{female}= \\frac{22}{28} = 0.79$, hence we have:\n",
    "<center>$\n",
    "\\left\\{ \n",
    "  \\begin{aligned}\n",
    "    AOD = \\frac{1}{2} [(0.18-0.41)+(0.79-0.91)] = -0.175 \\\\\n",
    "    EOD = 0.79-0.91 = -0.12 \n",
    "  \\end{aligned}\n",
    "\\right. $ </center>\n",
    "\n",
    "- **Challenges/Uses in practice**: This assumes that the ground-truth label is observed and unbiased for all individuals. In hiring for instance, the best available ground-truth label might be biased or unobserved, for instance it isimpossible to know if all historically rejected candidates would have been good employees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPB2uV50H7kZ"
   },
   "source": [
    "# 5 - Which definition to choose ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyvZAmFrH7kZ"
   },
   "source": [
    "It is recognized that it is mathematically impossible to satisfy all common fairness definitions at the same time. There are numerous studies illustrating this and we refer the reader to the following: [[Kleinberg et al., 2016]](https://arxiv.org/pdf/1609.05807.pdf)  and [[Chouldechova, 2017]](https://arxiv.org/pdf/1610.07524.pdf). For a specific problem, we therefore have to choose an dapted definition of fairness. \n",
    "\n",
    "There is no easy way of doing this. Ideally,deciding which definition to use must be done in accordance with governance structures. In practice there is little guidance and this is decided on a case-by-case basis and through best practice in specific indutries. \n",
    "\n",
    "\n",
    "\n",
    "[This 2019 paper](https://arxiv.org/pdf/1912.06883.pdf) from Binns also looks at the apparent conflict between individual and group fairness on a more conceptual level. \n",
    "\n",
    "Individual concepts of fairness have limited applicability in practice and are rarely used. Indeed, leaving out protected attributes is often not enough (Fairness through Unawareness) because of the presence of proxies in the data, and the two other methods can be difficult to implement: it is not easy to come up and agree on a similarity measure between individuals for Fairness Through Awareness, and similarly to construct a causal graph for Counterfactual Fairness, especially as the number of features increases.\n",
    "\n",
    "In the group fairness space, the debate is between using an Equality of Outcome or Equality of Opportunity approach. The paper by [Friedler et al., 2016](https://arxiv.org/pdf/1609.07236.pdf?source=post_page%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D%2D-) gives an interesting perspective. It introduces the concept of \"worldviews\" and states that there are two worldviews that dictates the concept of fairness to be used. Before presenting these wordlview, we need to present their idea of an \"observed space\" and \"construct space\". The \"observed space\" contain the actual observable features of the individuals that we have at hands to make a dieision, but that may contain biases. The \"construct space\" is an hypothetical ideal and unobservable space that contain the *true features* of each inividuals, once the observable space is freed from all biases. \n",
    "\n",
    "The figure below present the observed space (with Black and White individuals separated by a line as an illustration) and construct space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/cheatsheet_img4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two worldviews are:\n",
    "- Worldview \"What you see is what you get\" (WYSIWYG). This states that the observed space is essentially the same as the construct space, hence individuals can be directly compared using the observed features and labels. This re-joins the concept of Fairness Through Awareness, but also the Equality of Opportunity definitions of fairness where we just seek for the model to be equally accurate or inaccurate for all groups. \n",
    "- Worldview \"We're all equal\" (WAE).  Here we assume that the observed space is not a good measure of the construct space as there exists structural bias. This re-joins the concept of Equaility of Outcome. \n",
    "\n",
    "The paper illustrates these two wordview using the SAT example:\n",
    "- \"Worldview says that the score correlates well with future success and there is a way to use the score to correctly compare the abilities of applicants\". -> use equality of opportunity metrics such as Average Odds Difference. \n",
    "- \"Worldview says that the SAT score may contain structural biases so its distribution being different across groups should not be mistaken for a difference in distribution in ability.\" -> use equality of outcome metrics such as Disparate Impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Milestone3_Theory_Supporting Notebook_CheatSheet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
