{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "M3_Practice_1_Explore data.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "-nYt5_h77QGN",
    "2b-UuCXc79U1",
    "67rZSU6hDQwj",
    "QabzuO7TI43r",
    "RU4xvzSuMBRf",
    "7v5PQTdLcpo5",
    "u253OROG_dyz",
    "_Mv-q_mLpiGx"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMuRYKtj4SHR"
   },
   "source": [
    "# **Milestone 3: Sources, Forms, and Quantification of Bias and Discrimination in Supervised Learning**\n",
    "# **PRACTICE NOTEBOOK 1 - Explore given data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0v83HDaM4g99"
   },
   "source": [
    "In this part of the course, we will look for bias using a practical example. A  company is looking to hire a new employee. They use a machine learning algorithm to select the top candidates. The candidates are assigned either 0 if they're not selected or 1 if they are. \n",
    "\n",
    "There are 4 practice notebooks in total (current one in red):\n",
    "1. <font color='red'> **Explore given data: label distribution & check for proxies**</font> \n",
    "2. Evaluate model bias \"manually\"\n",
    "3. Evaluate model bias using existing libraries (aif360)\n",
    "4. Example code to get confidence intervals for a metric (nothing to do)\n",
    "\n",
    "Instructions to complete in each parts are in bold. Intermediate results are given so one can continue the exercise. \n",
    "\n",
    "This is notebook number 1. \n",
    "\n",
    "In this notebook, we:\n",
    "- Import modules, define some useful functions, and load the data\n",
    "- Check for nan values\n",
    "- Explore the demographics of the candidates and the label distribution per group\n",
    "- Look for proxies in the data\n",
    "- See if we can build a model that \"guess\" the gender/ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nYt5_h77QGN"
   },
   "source": [
    "## **0 - Import modules, load data and useful functions**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KgfZWXdy1REj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7ef080a0-9121-4085-bf2c-a0f35dce19d5"
   },
   "source": [
    "#imports \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQ0R86MF7trN",
    "outputId": "d953dba9-6866-4902-87d2-0e784722d7c5"
   },
   "source": [
    "# Only run if running on Google Colab\n",
    "!pip3 install pickle5\n",
    "import pickle5 as pickle\n",
    "!gdown --id 1-Wd1evAoDs4YsjRLfC-ifarmQL-Ozg3R # download data file from public link and place it in content/ folder"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pickle5\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
      "\u001B[K     |████████████████████████████████| 132 kB 3.3 MB/s \n",
      "\u001B[?25hBuilding wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219316 sha256=acdecb8c4508c3e32548a7f1057606ce1884aab847b21078c445c3982fc20a63\n",
      "  Stored in directory: /root/.cache/pip/wheels/7e/6a/00/67136a90d6aca437d806d1d3cedf98106e840c97a3e5188198\n",
      "Successfully built pickle5\n",
      "Installing collected packages: pickle5\n",
      "Successfully installed pickle5-0.0.11\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-Wd1evAoDs4YsjRLfC-ifarmQL-Ozg3R\n",
      "To: /content/data.pickle\n",
      "128MB [00:01, 125MB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "ClEzlOOOpr0B",
    "outputId": "d9c3d4b6-037e-4c1d-8380-3a4d2df42704"
   },
   "source": [
    "'''\n",
    "Uncomment the code below if you are running this from your local machine\n",
    "Note: Place data.pickle file in the same folder as the .ipynb notebook (Download link: https://docs.google.com/uc?export=download&id=1-Wd1evAoDs4YsjRLfC-ifarmQL-Ozg)\n",
    "'''\n",
    "# with open('data.pickle', 'rb') as handle:\n",
    "#     raw_data = pickle.load(handle)   \n",
    "if 'google.colab' in str(get_ipython()): # if running from colab\n",
    "  with open('/content/data.pickle', 'rb') as handle:\n",
    "      raw_data = pickle.load(handle)  \n",
    "raw_data[:5] #display the first 5 candidates data"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>...</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>28.021737</td>\n",
       "      <td>4.351153</td>\n",
       "      <td>2.453895</td>\n",
       "      <td>1.637143</td>\n",
       "      <td>-1.746628</td>\n",
       "      <td>-0.483463</td>\n",
       "      <td>0.034170</td>\n",
       "      <td>1.399225</td>\n",
       "      <td>-0.795440</td>\n",
       "      <td>0.417474</td>\n",
       "      <td>0.214564</td>\n",
       "      <td>-0.471581</td>\n",
       "      <td>1.945645</td>\n",
       "      <td>-0.676217</td>\n",
       "      <td>1.213878</td>\n",
       "      <td>0.015701</td>\n",
       "      <td>1.472670</td>\n",
       "      <td>-0.054158</td>\n",
       "      <td>0.106858</td>\n",
       "      <td>-1.073194</td>\n",
       "      <td>-1.071848</td>\n",
       "      <td>-0.249942</td>\n",
       "      <td>0.634626</td>\n",
       "      <td>-0.732358</td>\n",
       "      <td>2.445728</td>\n",
       "      <td>0.784284</td>\n",
       "      <td>0.112329</td>\n",
       "      <td>1.055362</td>\n",
       "      <td>-0.605459</td>\n",
       "      <td>1.259140</td>\n",
       "      <td>-0.287927</td>\n",
       "      <td>0.214142</td>\n",
       "      <td>-0.644585</td>\n",
       "      <td>1.165376</td>\n",
       "      <td>-0.409198</td>\n",
       "      <td>-0.705823</td>\n",
       "      <td>0.091147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655416</td>\n",
       "      <td>-0.012623</td>\n",
       "      <td>0.660826</td>\n",
       "      <td>0.258141</td>\n",
       "      <td>0.036875</td>\n",
       "      <td>-0.229340</td>\n",
       "      <td>0.353817</td>\n",
       "      <td>-0.178814</td>\n",
       "      <td>-0.145229</td>\n",
       "      <td>-0.040692</td>\n",
       "      <td>-0.046980</td>\n",
       "      <td>0.311939</td>\n",
       "      <td>-0.348202</td>\n",
       "      <td>0.271357</td>\n",
       "      <td>0.355443</td>\n",
       "      <td>-0.050447</td>\n",
       "      <td>-0.051816</td>\n",
       "      <td>0.083028</td>\n",
       "      <td>0.184139</td>\n",
       "      <td>0.107824</td>\n",
       "      <td>-0.083415</td>\n",
       "      <td>-0.359288</td>\n",
       "      <td>0.156547</td>\n",
       "      <td>-0.588539</td>\n",
       "      <td>-0.025777</td>\n",
       "      <td>-0.172269</td>\n",
       "      <td>0.331421</td>\n",
       "      <td>0.222768</td>\n",
       "      <td>-0.319124</td>\n",
       "      <td>-0.060476</td>\n",
       "      <td>-0.557444</td>\n",
       "      <td>-0.015627</td>\n",
       "      <td>-0.052749</td>\n",
       "      <td>-0.234189</td>\n",
       "      <td>-0.072384</td>\n",
       "      <td>0.090403</td>\n",
       "      <td>0.376761</td>\n",
       "      <td>0.258914</td>\n",
       "      <td>-0.050558</td>\n",
       "      <td>0.014513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>29.603342</td>\n",
       "      <td>-3.407193</td>\n",
       "      <td>0.771800</td>\n",
       "      <td>-2.957411</td>\n",
       "      <td>0.599226</td>\n",
       "      <td>-2.805277</td>\n",
       "      <td>0.329414</td>\n",
       "      <td>-2.055339</td>\n",
       "      <td>-1.194446</td>\n",
       "      <td>-0.633159</td>\n",
       "      <td>2.268302</td>\n",
       "      <td>1.159443</td>\n",
       "      <td>0.899266</td>\n",
       "      <td>-0.472739</td>\n",
       "      <td>0.541605</td>\n",
       "      <td>-1.248643</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>1.225688</td>\n",
       "      <td>0.456477</td>\n",
       "      <td>-1.483071</td>\n",
       "      <td>-0.944882</td>\n",
       "      <td>1.483229</td>\n",
       "      <td>0.512809</td>\n",
       "      <td>0.692537</td>\n",
       "      <td>0.178988</td>\n",
       "      <td>-1.609531</td>\n",
       "      <td>-1.985852</td>\n",
       "      <td>-0.469491</td>\n",
       "      <td>-1.156583</td>\n",
       "      <td>0.475535</td>\n",
       "      <td>-0.041015</td>\n",
       "      <td>-0.214832</td>\n",
       "      <td>-0.681641</td>\n",
       "      <td>1.131433</td>\n",
       "      <td>-0.667814</td>\n",
       "      <td>0.267111</td>\n",
       "      <td>-0.112433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034605</td>\n",
       "      <td>-0.024487</td>\n",
       "      <td>-0.212205</td>\n",
       "      <td>0.440684</td>\n",
       "      <td>-0.065303</td>\n",
       "      <td>0.409878</td>\n",
       "      <td>0.455144</td>\n",
       "      <td>-0.108402</td>\n",
       "      <td>0.027760</td>\n",
       "      <td>-0.015238</td>\n",
       "      <td>0.027453</td>\n",
       "      <td>0.319960</td>\n",
       "      <td>-0.014589</td>\n",
       "      <td>-0.083241</td>\n",
       "      <td>-0.285702</td>\n",
       "      <td>0.047510</td>\n",
       "      <td>-0.144107</td>\n",
       "      <td>0.405289</td>\n",
       "      <td>-0.044139</td>\n",
       "      <td>-0.287215</td>\n",
       "      <td>0.201876</td>\n",
       "      <td>-0.298703</td>\n",
       "      <td>0.347969</td>\n",
       "      <td>0.029646</td>\n",
       "      <td>0.073052</td>\n",
       "      <td>-0.010259</td>\n",
       "      <td>0.023681</td>\n",
       "      <td>0.373202</td>\n",
       "      <td>-0.525402</td>\n",
       "      <td>-0.198727</td>\n",
       "      <td>-0.198440</td>\n",
       "      <td>-0.158843</td>\n",
       "      <td>0.191984</td>\n",
       "      <td>-0.004532</td>\n",
       "      <td>0.229210</td>\n",
       "      <td>-0.173042</td>\n",
       "      <td>-0.072871</td>\n",
       "      <td>0.442939</td>\n",
       "      <td>-0.054423</td>\n",
       "      <td>0.026959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>26.504283</td>\n",
       "      <td>0.642464</td>\n",
       "      <td>2.522944</td>\n",
       "      <td>-2.197094</td>\n",
       "      <td>2.270646</td>\n",
       "      <td>-0.472510</td>\n",
       "      <td>0.532815</td>\n",
       "      <td>-0.266449</td>\n",
       "      <td>-0.131638</td>\n",
       "      <td>1.038315</td>\n",
       "      <td>-0.865827</td>\n",
       "      <td>-0.811267</td>\n",
       "      <td>-0.381401</td>\n",
       "      <td>-0.801701</td>\n",
       "      <td>-0.485021</td>\n",
       "      <td>0.656005</td>\n",
       "      <td>2.489571</td>\n",
       "      <td>-0.714447</td>\n",
       "      <td>0.658228</td>\n",
       "      <td>-0.075957</td>\n",
       "      <td>-1.159888</td>\n",
       "      <td>-2.334786</td>\n",
       "      <td>-0.253364</td>\n",
       "      <td>-2.073697</td>\n",
       "      <td>-0.939994</td>\n",
       "      <td>-1.177166</td>\n",
       "      <td>0.551689</td>\n",
       "      <td>-1.313316</td>\n",
       "      <td>-0.486217</td>\n",
       "      <td>0.732130</td>\n",
       "      <td>-0.320456</td>\n",
       "      <td>-1.143053</td>\n",
       "      <td>1.297522</td>\n",
       "      <td>-0.617038</td>\n",
       "      <td>0.340978</td>\n",
       "      <td>0.978603</td>\n",
       "      <td>0.398515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510825</td>\n",
       "      <td>-0.616479</td>\n",
       "      <td>0.644675</td>\n",
       "      <td>0.505319</td>\n",
       "      <td>-0.299894</td>\n",
       "      <td>-0.058435</td>\n",
       "      <td>0.095024</td>\n",
       "      <td>-0.101136</td>\n",
       "      <td>0.042583</td>\n",
       "      <td>0.061005</td>\n",
       "      <td>0.304137</td>\n",
       "      <td>0.259210</td>\n",
       "      <td>-0.022425</td>\n",
       "      <td>0.138097</td>\n",
       "      <td>-0.442536</td>\n",
       "      <td>-0.108350</td>\n",
       "      <td>0.369865</td>\n",
       "      <td>0.151049</td>\n",
       "      <td>0.096285</td>\n",
       "      <td>0.013651</td>\n",
       "      <td>0.175281</td>\n",
       "      <td>0.144344</td>\n",
       "      <td>-0.006250</td>\n",
       "      <td>0.100850</td>\n",
       "      <td>-0.051642</td>\n",
       "      <td>0.122977</td>\n",
       "      <td>-0.088661</td>\n",
       "      <td>-0.229844</td>\n",
       "      <td>-0.272144</td>\n",
       "      <td>0.012633</td>\n",
       "      <td>0.423352</td>\n",
       "      <td>-0.033844</td>\n",
       "      <td>-0.125387</td>\n",
       "      <td>-0.483924</td>\n",
       "      <td>-0.116553</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.017111</td>\n",
       "      <td>-0.012309</td>\n",
       "      <td>0.264572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>25.012088</td>\n",
       "      <td>0.895121</td>\n",
       "      <td>-2.092517</td>\n",
       "      <td>3.687830</td>\n",
       "      <td>0.539642</td>\n",
       "      <td>1.988930</td>\n",
       "      <td>1.121646</td>\n",
       "      <td>2.255337</td>\n",
       "      <td>-0.128801</td>\n",
       "      <td>1.148379</td>\n",
       "      <td>1.616247</td>\n",
       "      <td>-2.599757</td>\n",
       "      <td>-0.322807</td>\n",
       "      <td>2.102508</td>\n",
       "      <td>-0.204551</td>\n",
       "      <td>0.069818</td>\n",
       "      <td>0.745222</td>\n",
       "      <td>-0.859875</td>\n",
       "      <td>-2.235995</td>\n",
       "      <td>-0.207436</td>\n",
       "      <td>-1.678697</td>\n",
       "      <td>-0.569024</td>\n",
       "      <td>-0.723122</td>\n",
       "      <td>-0.144833</td>\n",
       "      <td>-1.537487</td>\n",
       "      <td>1.678429</td>\n",
       "      <td>0.501249</td>\n",
       "      <td>-0.230747</td>\n",
       "      <td>0.746559</td>\n",
       "      <td>-0.069959</td>\n",
       "      <td>-0.346651</td>\n",
       "      <td>0.448291</td>\n",
       "      <td>0.283592</td>\n",
       "      <td>-0.445759</td>\n",
       "      <td>-0.529080</td>\n",
       "      <td>0.287333</td>\n",
       "      <td>0.466766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153845</td>\n",
       "      <td>-0.049137</td>\n",
       "      <td>-0.023112</td>\n",
       "      <td>-0.173509</td>\n",
       "      <td>-0.265363</td>\n",
       "      <td>0.091898</td>\n",
       "      <td>0.016743</td>\n",
       "      <td>-0.092894</td>\n",
       "      <td>-0.009915</td>\n",
       "      <td>-0.031731</td>\n",
       "      <td>0.153983</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.123019</td>\n",
       "      <td>-0.035719</td>\n",
       "      <td>-0.045633</td>\n",
       "      <td>-0.103204</td>\n",
       "      <td>0.089567</td>\n",
       "      <td>0.104990</td>\n",
       "      <td>0.337228</td>\n",
       "      <td>-0.018783</td>\n",
       "      <td>-0.215437</td>\n",
       "      <td>0.268139</td>\n",
       "      <td>-0.125425</td>\n",
       "      <td>0.095183</td>\n",
       "      <td>-0.125172</td>\n",
       "      <td>-0.226467</td>\n",
       "      <td>0.371647</td>\n",
       "      <td>-0.023041</td>\n",
       "      <td>-0.093040</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>-0.280392</td>\n",
       "      <td>0.046582</td>\n",
       "      <td>0.116709</td>\n",
       "      <td>0.133876</td>\n",
       "      <td>0.072716</td>\n",
       "      <td>0.124083</td>\n",
       "      <td>0.213735</td>\n",
       "      <td>-0.149901</td>\n",
       "      <td>-0.217130</td>\n",
       "      <td>0.004403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>27.358934</td>\n",
       "      <td>-2.332423</td>\n",
       "      <td>0.154999</td>\n",
       "      <td>-2.623793</td>\n",
       "      <td>1.682456</td>\n",
       "      <td>1.262280</td>\n",
       "      <td>-1.685565</td>\n",
       "      <td>0.489319</td>\n",
       "      <td>-0.043471</td>\n",
       "      <td>-0.372265</td>\n",
       "      <td>1.778535</td>\n",
       "      <td>-1.145419</td>\n",
       "      <td>2.461327</td>\n",
       "      <td>1.396318</td>\n",
       "      <td>-0.911969</td>\n",
       "      <td>-2.228570</td>\n",
       "      <td>1.378633</td>\n",
       "      <td>-1.512325</td>\n",
       "      <td>-0.440331</td>\n",
       "      <td>-0.111163</td>\n",
       "      <td>-0.885884</td>\n",
       "      <td>-0.840501</td>\n",
       "      <td>1.576620</td>\n",
       "      <td>-0.972075</td>\n",
       "      <td>-2.008346</td>\n",
       "      <td>-0.358732</td>\n",
       "      <td>0.896535</td>\n",
       "      <td>0.562193</td>\n",
       "      <td>0.154542</td>\n",
       "      <td>-1.077315</td>\n",
       "      <td>1.902062</td>\n",
       "      <td>1.728109</td>\n",
       "      <td>0.317205</td>\n",
       "      <td>-0.436143</td>\n",
       "      <td>0.226549</td>\n",
       "      <td>-0.502206</td>\n",
       "      <td>-0.157102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181831</td>\n",
       "      <td>-0.026589</td>\n",
       "      <td>-0.051453</td>\n",
       "      <td>0.261819</td>\n",
       "      <td>-0.048644</td>\n",
       "      <td>-0.099526</td>\n",
       "      <td>-0.026777</td>\n",
       "      <td>0.039836</td>\n",
       "      <td>-0.168277</td>\n",
       "      <td>0.077232</td>\n",
       "      <td>0.193722</td>\n",
       "      <td>0.093298</td>\n",
       "      <td>-0.075132</td>\n",
       "      <td>-0.063202</td>\n",
       "      <td>0.120167</td>\n",
       "      <td>0.039270</td>\n",
       "      <td>0.350429</td>\n",
       "      <td>0.166559</td>\n",
       "      <td>0.130134</td>\n",
       "      <td>-0.181019</td>\n",
       "      <td>-0.193276</td>\n",
       "      <td>0.312204</td>\n",
       "      <td>-0.187331</td>\n",
       "      <td>-0.029194</td>\n",
       "      <td>-0.212277</td>\n",
       "      <td>-0.463872</td>\n",
       "      <td>0.041810</td>\n",
       "      <td>0.041185</td>\n",
       "      <td>-0.182479</td>\n",
       "      <td>-0.182461</td>\n",
       "      <td>-0.019350</td>\n",
       "      <td>-0.093371</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>-0.025467</td>\n",
       "      <td>0.155397</td>\n",
       "      <td>-0.067609</td>\n",
       "      <td>-0.084833</td>\n",
       "      <td>0.033429</td>\n",
       "      <td>-0.199198</td>\n",
       "      <td>0.229629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Gender Ethnicity          0  ...       496       497       498       499\n",
       "0      0  Female     White  28.021737  ...  0.376761  0.258914 -0.050558  0.014513\n",
       "1      0  Female     White  29.603342  ... -0.072871  0.442939 -0.054423  0.026959\n",
       "2      1  Female  Hispanic  26.504283  ...  0.015519  0.017111 -0.012309  0.264572\n",
       "3      0  Female  Hispanic  25.012088  ...  0.213735 -0.149901 -0.217130  0.004403\n",
       "4      1    Male  Hispanic  27.358934  ... -0.084833  0.033429 -0.199198  0.229629\n",
       "\n",
       "[5 rows x 503 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PLwniYoCY6_y"
   },
   "source": [
    "def plot_cm(y_true,y_pred,labels = [1,0],display_labels = [1,0], ax = None):\n",
    "  cm = confusion_matrix(y_true,y_pred,labels = labels)\n",
    "  if ax is None:\n",
    "    fig, ax = plt.subplots()\n",
    "  else:\n",
    "    fig = ax.figure\n",
    "  sns.heatmap(cm, annot=True, ax = ax, cmap='viridis',fmt='g')\n",
    "\n",
    "  ax.set(xticklabels=display_labels,\n",
    "          yticklabels=display_labels,\n",
    "          ylabel=\"True label\",\n",
    "          xlabel=\"Predicted label\")\n",
    "  return cm\n",
    "def split_data_from_df(data):\n",
    "  y = data['Label'].values\n",
    "  # g = data['Gender'].values\n",
    "  # e = data['Ethnicity'].values\n",
    "  X = data[np.arange(500)].values\n",
    "  filter_col = ['Ethnicity','Gender'] + [col for col in data if str(col).startswith('Ethnicity_')] + [col for col in data if str(col).startswith('Gender_')] \n",
    "  dem = data[filter_col].copy()\n",
    "  return X,y,dem\n",
    "def encode(df):\n",
    "  g_enc = LabelEncoder()\n",
    "  e_enc = LabelEncoder()\n",
    "  df['Gender'] = g_enc.fit_transform(df['Gender'])\n",
    "  df['Ethnicity'] = e_enc.fit_transform(df['Ethnicity'])\n",
    "  return df, g_enc,e_enc\n",
    "def resample_equal(df,cat):\n",
    "  df['uid'] = df[cat] + df['Label'].astype(str)\n",
    "  enc = LabelEncoder()\n",
    "  df['uid'] = enc.fit_transform(df['uid'])\n",
    "  # Resample\n",
    "  uid = df['uid'].values\n",
    "  res = imblearn.over_sampling.RandomOverSampler(random_state=6)\n",
    "  df_res,euid = res.fit_resample(df,uid)\n",
    "  df_res = pd.DataFrame(df_res,columns = df.columns)\n",
    "  df_res = df_res.sample(frac=1).reset_index(drop=True)\n",
    "  df_res['Label'] = df_res['Label'].astype(float)\n",
    "  return df_res"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b-UuCXc79U1"
   },
   "source": [
    "## **1- Data presentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71RjuIRAsYwl"
   },
   "source": [
    "Each row in the data represent a candidate. There are 3 components to the data:\n",
    "- y : the \"ground-truth\" label in the \"Label\" column. This is whether the candidate should actually receive a positive or negative rating (repsectively 1 or 0).\n",
    "- X : the input features in the columns 0 to 499. There are 500 features for each candidates, extracted from resume and video interview.\n",
    "- dem : the demographic information in the columns \"Gender\" or \"Ethnicity\". \n",
    "\n",
    "We extract them from a given dataframe using the pre-defined function \"split_data_from_df\" to be used as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "lpItDrXHuE6L",
    "outputId": "1ecd5b98-9f92-4ff3-a881-63b3d9543cc0"
   },
   "source": [
    "data = raw_data.copy()\n",
    "X,y,dem = split_data_from_df(data)\n",
    "print(X.shape, y.shape)\n",
    "display(dem.head(3))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(31771, 500) (31771,)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ethnicity  Gender\n",
       "0     White  Female\n",
       "1     White  Female\n",
       "2  Hispanic  Female"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37vXWJT7piGr"
   },
   "source": [
    "In this section we will:\n",
    "1. Look for nan values\n",
    "2. Explore the demographics of the dataset and distribution of labels\n",
    "3. Check for proxies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67rZSU6hDQwj"
   },
   "source": [
    "## **2. Nan values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74DWLk9IDH92"
   },
   "source": [
    "**Questions** : \n",
    "- **Check for the number of nan values in each column of *data*.** <br>\n",
    "    *Expected results*: No nan values in the features or labels. 3997 nan values each for columns Gender and Ethnicity.\n",
    "\n",
    "\n",
    "- **How are these nan values distributed ?** \n",
    "Are the people missing gender information the same as the ones missing ethnicity information ? You can use the missingno library to visualise this on the *dem* dataframe ([see demo here](https://www.geeksforgeeks.org/python-visualize-missing-values-nan-values-using-missingno-library/)). Check your assumptions numerically. <br>\n",
    "    *Expected results*: You should find that for a given candidate, either both the gender or ethnicity is missing, either all of the information is there. We will call the group of candidates with missing information \"the nan group\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wMUs5xj06yP0"
   },
   "source": [
    "data = raw_data.copy() # work with the \"data\" dataframe (simple copy of the raw dataframe)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "10MtLhW9D_qS"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8CzlOtdMD_w-"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QabzuO7TI43r"
   },
   "source": [
    "## **3. Demographics of the dataset and average ground-truth success rate for each group**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzXlWykkEe18"
   },
   "source": [
    "**Questions** :\n",
    "- **What groups are represented for gender/ethnicity?** Draw histograms of the *Gender* and *Ethnicity* columns in *data*.\n",
    "- **Exploring the labels in y, what proportions of all the candidates population receives a positive outcome ?**\n",
    "- **Now let's calculate the proportion of positive outcome (or success rate) for each gender and ethnicity.** \n",
    "\n",
    "Notes:\n",
    "- Because the label is 0 or 1, calculating the proportion of positive outcome is the same as calculating the mean y values. \n",
    "- For this section, we will replace the nan values in the *Gender* and *Ethnicity* columns with a string so that our code will treat these candidates as if they were their own group.\n",
    "- Usual histogram plotting functions ignore nan values. Because we have changed these into strings, the nan group won't be ignored here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bCRlQauxT1Gx"
   },
   "source": [
    "# replace nan values with 'nan' string\n",
    "data = raw_data.copy() # work with the \"data\" dataframe (simple copy of the raw dataframe)\n",
    "data = data.fillna(\"nan\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J1ftZ2dZEEBb"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2rmRWcF1EEDz"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQUWxpIDKgI1"
   },
   "source": [
    "You should find the following success rates per group:\n",
    "\n",
    "\n",
    "| Group | Success Rate |\n",
    "| -: | :-: |\n",
    "**Mean score per gender**\n",
    "| Female |  0.383265 |\n",
    "| Male |  0.384994 |\n",
    "|nan      |   0.308982 |\n",
    "|**Mean score per ethnicity**\n",
    "|Asian     |  0.339046|\n",
    "| Black    |   0.363655|\n",
    "|Hispanic   | 0.384433|\n",
    "|White     |  0.410793|\n",
    "|nan      |   0.308982|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WqJ9UdtpiGu"
   },
   "source": [
    "**Questions** :\n",
    "- **Ignoring the nan group for now, can you guess which group might experience the most bias with a model trained on this data ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k1BpzpVLA2o"
   },
   "source": [
    "    Answer :  Asian\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMYO26_0UkjM"
   },
   "source": [
    "**Note on the nan group :** \n",
    "As seen in the results, the 'nan' group has the lowest success rate. If we continue the fairness analysis by leaving it in, we will see that it is also the group against which there is the strongest bias. Because we have no prior information on how the \"nan\" group was constituted and who is in it, it is difficult to draw any conclusion on what this means. In an ideal world, there should be no subgroups constituted at random against which there's an observable bias. However, we do not know whether this group is randomly constituted or not. Possible reasons for missing data could be: (i) people opted out of providing their ethnicity or gender, (ii) these features could have been reverse-engineered from a photo or video and there was no video available for these candidates, or (iii) a reason that correlates somehow with less qualified candidates. \n",
    "\n",
    "In the remaining of this course, we remove the nan group from the analysis for simplicity. In a real-life context, the reason behind the bias on the nan group should be investigated and mitigating this bias could be needed. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7EmGsg4raHgi"
   },
   "source": [
    "# remove all nans\n",
    "data = data[data['Gender']!= 'nan'].copy()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU4xvzSuMBRf"
   },
   "source": [
    "## **4. Looking for proxy features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31hKwwU8MJp-"
   },
   "source": [
    "The features do not include explicitely any protected characteristics related to gender or ethnicity. It can de facto be deemed fair with respect to the *Fairness Through Unawareness* definition. However sometimes it is possible to find proxies for protected attributes in the data, meaning that a specific feature might be strongly correlated with a protected characteristic. \n",
    "\n",
    "We can check whether a particular feature is highly correlated with ethnicity or gender by calculating the pearson correlation coefficients of each feature with the gender or ethnicity. Let's demonstrate this for gender only. The same can be done for ethnicity as an exercise.\n",
    "\n",
    "The following code extracts from the data:\n",
    "\n",
    "- the input features X\n",
    "- the gender g\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGKuYQtC9lTD",
    "outputId": "6df814ad-556f-4ddc-ed1e-d69c631d2c78"
   },
   "source": [
    "# copy raw data and drop nan values\n",
    "df = raw_data.copy()\n",
    "df = df.dropna()\n",
    "# Extract X and g\n",
    "X,_,dem = split_data_from_df(df)\n",
    "g = dem['Gender'].values\n",
    "enc = LabelEncoder()\n",
    "g = enc.fit_transform(g)\n",
    "print(\"Label for Male is %d. Label for Female is %d\"%(enc.transform(['Male'])[0],enc.transform(['Female'])[0]))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label for Male is 1. Label for Female is 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ocf11GDj940s"
   },
   "source": [
    "**Questions** : \n",
    "- **Calculate the correlation between the gender g and all other features.**\n",
    "  - using scipy.stats.pearsonr get the correlation coefficient between g and each feature. You can for instance use a loop to go through each features and store results in a list or 1D array. \n",
    "  - plot a graph with the correlation coefficient on the y axis and features on the X axis. \n",
    "- **Is there any feature correlating highly with gender?**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f025THi-EG0l"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S2C1smbjEG_l"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PGijvAK9lpY"
   },
   "source": [
    "## **5. Can we still predict the protected characteristic using the entire set of features?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQTbcku99aOd"
   },
   "source": [
    "The previous section should show that there is no single feature that is highly correlated with gender in this case. \n",
    "\n",
    "However, can we still infer the gender or ethnicity of a participant from the features themselves? We will try to build a model that predicts the gender or ethnicity from the features.\n",
    "\n",
    "In this section we use a simple RidgeClassifier to try and do this. The following code defines the classifier, and prepare the data for the task. We will try to predict whether the candidate is a Female or Male. \n",
    "\n",
    "As an optional exercise, you may also repeat that exercise for Black/White ethnicity. This is interesting as the dataset is imbalanced regarding these groups (many more White candidates than Black), hence the results are not as straightforward as for Male/Female."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v5PQTdLcpo5"
   },
   "source": [
    "### **5.1. Predict Male/Female gender**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HvqdcGrZwMC"
   },
   "source": [
    "In the following code we:\n",
    "- encode the *Gender* column into integers\n",
    "- split the data into train and test set. For this section you will need: \n",
    "  - X_train , X_test. are the train and test input features \n",
    "  - g_train, g_test. contain the encoded gender (0 for Female; 1 for Male) and are used as labels in this section\n",
    "- initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RluGTRSTQKw",
    "outputId": "66641a0b-37ab-4882-e94f-801fb25d50bb"
   },
   "source": [
    "# copy raw data and drop nan values\n",
    "df = raw_data.copy()\n",
    "df = df.dropna()\n",
    "# encode 'Gender' column\n",
    "enc = LabelEncoder()\n",
    "df['Gender'] = enc.fit_transform(df['Gender'])\n",
    "print(\"Label for Male is %d. Label for Female is %d\"%(enc.transform(['Male'])[0],enc.transform(['Female'])[0]))\n",
    "# split data train/test (from data without nan)\n",
    "data_train, data_test = train_test_split(df,test_size = 0.3,random_state=41)\n",
    "# get X,y, gender, ethnicity\n",
    "X_train,_,dem_train = split_data_from_df(data_train)\n",
    "g_train = dem_train['Gender']\n",
    "X_test,_,dem_test = split_data_from_df(data_test)\n",
    "g_test = dem_test['Gender']\n",
    "# define model\n",
    "model = RidgeClassifier(random_state = 42)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label for Male is 1. Label for Female is 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5NcbyU7cw-r"
   },
   "source": [
    "**Instructions:**\n",
    "- **Fit the model on the train set**\n",
    "- **Compute the accuracy and confusion matrix on the test set.**  You can use *accuracy_score* (sklearn.metrics) and *plot_cm* (defined in 1)\n",
    "- **Check the precision, recall and f1 score for each class.** You can use *precision_recall_fscore_support* (sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rxZUGmStEIgC"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gwtDAvxZEIjU"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UhgsjiMXEIsh"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DG9NoUtOpiGv"
   },
   "source": [
    "You should obtain the following results:\n",
    "\n",
    "Overall Accuracy: 0.70\n",
    "\n",
    "| Group | Precision | Recall | f1 |\n",
    "| --- | --- | --- | --- | \n",
    "| Female | 0.70| 0.68 | 0.69 |\n",
    "| Male |  0.71| 0.73 | 0.72 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlDwo5dGpiGw"
   },
   "source": [
    "**Questions:**\n",
    "- **With what accuracy is it possible to predict if the candidate is a Male or Female ? How does that compare with a random guess ?**\n",
    "- **What does the precision and recall score mean ? Is there any class imbalance affecting the model ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5w97rKrvETE-"
   },
   "source": [
    "    Answer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5BfPCEhbvb8"
   },
   "source": [
    "Some possible reasons behind the fact that the features enable gender classification are: \n",
    " - There are some video, photo or voice features that enable gender recognition, or any other ensemble of features that do the same.\n",
    " - There are some specific proxy variables that are correlated with gender, as for instance  \"Maternity leave\" or \"Employment length\". We have seen from section 4 that this was not the case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u253OROG_dyz"
   },
   "source": [
    "### **5.2. (OPTIONAL) Predict Black/White ethnicity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcA_RbypZ_6R"
   },
   "source": [
    "In the following code, we perform the same steps as for gender:\n",
    "- only keep *White* and *Black* candidates\n",
    "- encode the *Ethnicity* column into integers\n",
    "- split the data into train and test set. For this section you will need: \n",
    "  - X_train , X_test. are the train and test input features \n",
    "  - e_train, e_test. contain the encoded ethnicity (0 for Black; 1 for White)\n",
    "- initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YozaeZP-Z_KY",
    "outputId": "b38bc183-4073-4266-af1b-59e31343015c"
   },
   "source": [
    "# copy raw data and drop nan values\n",
    "df = raw_data.copy()\n",
    "df = df.dropna()\n",
    "# remove non Black or non White for the purpose of this exercise\n",
    "df = data[(data['Ethnicity'] == 'White') | (data['Ethnicity'] == 'Black')].copy()\n",
    "# encode 'Ethnicity' column\n",
    "enc = LabelEncoder()\n",
    "df['Ethnicity'] = enc.fit_transform(df['Ethnicity'])\n",
    "print(\"Label for White is %d. Label for Black is %d\"%(enc.transform(['White'])[0],enc.transform(['Black'])[0]))\n",
    "# split data train/test (from data without nan)\n",
    "data_train, data_test = train_test_split(df,test_size = 0.3,random_state=41)\n",
    "# get X,y, gender, ethnicity\n",
    "X_train,_,dem_train = split_data_from_df(data_train)\n",
    "e_train = dem_train['Ethnicity']\n",
    "X_test,_,dem_test = split_data_from_df(data_test)\n",
    "e_test = dem_test['Ethnicity']\n",
    "# define model\n",
    "model = RidgeClassifier(random_state = 42)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label for White is 1. Label for Black is 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ1X4gWFdoEf"
   },
   "source": [
    "**Instructions.** As in the gender case: \n",
    "- **Fit the model on the train set**\n",
    "- **Compute the accuracy and confusion matrix on the test set.**  You can use *accuracy_score* (sklearn.metrics) and *plot_cm* (defined in 1)\n",
    "- **Check the precision, recall and f1 score for each class.** You can use *precision_recall_fscore_support* (sklearn.metrics).\n",
    "- **Count the number of *Black* and *White* candidates in the test dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ctk3rM0hEl5h"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "97y1SdG1El8C"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_Hk8znJpiGx"
   },
   "source": [
    "You should obtain the following results:\n",
    "\n",
    "Overall Accuracy: 0.76\n",
    "\n",
    "\n",
    "| Group | Precision | Recall | f1 |\n",
    "| --- | --- | --- | --- | \n",
    "| Black | 0.68 | 0.49 | 0.57 |\n",
    "| White |  0.79| 0.89 | 0.84 |\n",
    "\n",
    "\n",
    "Test set counts: {*White*: 2997, *Black*: 1411}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSiWFC_a6Pu9"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1O_F5NbhbmnbXiySd63iF26IGD6YQZxCk\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ntk5ZGzppiGx"
   },
   "source": [
    "You should observe in the confusion matrix that there are significantly less prediction for Black than for White. This would even happen with a perfect model as there is less candidates categorized as *Black* than *White* in the dataset. In these cases, the precision and recall scores help to judge the quality of a model to predict for each class.\n",
    "\n",
    "Here we can see that the precision score for both class is around the accuracy score, but the recall score is significantly higher for the *White* class. This means that the model does not think many candidates are *Black* (it misses many), but when if classifies one as *Black* it is rather precise. To tackle this, we can resample the data, i.e. randomly oversampling the minority class or undersampling the majority class so there is the same amount of each class. In theory, one should only resample the training data as the outcome for \"real-life\" test data is not available so we cannot resample it. However for the purpose of this class, we will resample the whole data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Mv-q_mLpiGx"
   },
   "source": [
    "### **5.3. (OPTIONAL) Predict *Black/White* ethnicity after rebalancing the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HosgAqg7piGx"
   },
   "source": [
    "In the following code, we undersample the data and repeat the experiment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vws8HD3CpiGx",
    "outputId": "e7ed403f-4dad-4d78-ee45-67464330c8bb"
   },
   "source": [
    "# copy raw data and drop nan values\n",
    "df = raw_data.copy()\n",
    "df = df.dropna()\n",
    "# get data\n",
    "df = data[(data['Ethnicity'] == 'White') | (data['Ethnicity'] == 'Black')].copy()\n",
    "X,_,dem = split_data_from_df(df)\n",
    "e = dem['Ethnicity'].values\n",
    "# resample\n",
    "print('Original dataset counts', Counter(e))\n",
    "res = RandomUnderSampler(random_state=42)\n",
    "X,e = res.fit_resample(X,e)\n",
    "print('Resampled dataset counts', Counter(e))\n",
    "# encode ethnicity\n",
    "enc = LabelEncoder()\n",
    "e = enc.fit_transform(e)\n",
    "# split data train/test (from data without nan)\n",
    "X_train, X_test, e_train, e_test = train_test_split(X,e,test_size = 0.3,random_state=41)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original dataset counts Counter({'White': 9932, 'Black': 4760})\n",
      "Resampled dataset counts Counter({'Black': 4760, 'White': 4760})\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VlvKWv3piGx"
   },
   "source": [
    "**Instructions.** Once again: \n",
    "- **Fit the model on the train set**\n",
    "- **Compute the accuracy and confusion matrix on the test set.**  You can use *accuracy_score* (sklearn.metrics) and *plot_cm* (defined in 1)\n",
    "- **Check the precision, recall and f1 score for each class.** You can use *precision_recall_fscore_support* (sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4nY07fXXEyqs"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pZvtTCHlEywO"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTzpOnkupiGy"
   },
   "source": [
    "**Questions:**\n",
    "- **Is the class imbalance still there ? Can one predict the ethnicity of *White* and *Black* candidates using the input features?**"
   ]
  }
 ]
}